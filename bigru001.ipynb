{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 导入库\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-10-07T08:00:43.481230Z","iopub.execute_input":"2023-10-07T08:00:43.481863Z","iopub.status.idle":"2023-10-07T08:00:43.488230Z","shell.execute_reply.started":"2023-10-07T08:00:43.481815Z","shell.execute_reply":"2023-10-07T08:00:43.486908Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"BiGRU（双向门控循环单元）是一种改进的循环神经网络（RNN）结构，它由两个独立的GRU层组成，一个沿正向处理序列，另一个沿反向处理序列。这种双向结构使得BiGRU能够捕捉到序列中的长距离依赖关系，从而提高模型的性能。","metadata":{}},{"cell_type":"markdown","source":"GRU（门控循环单元）是一种RNN变体，它通过引入更新门和重置门来解决传统RNN中的梯度消失问题。更新门负责确定何时更新隐藏状态，而重置门负责确定何时允许过去的信息影响当前隐藏状态。","metadata":{}},{"cell_type":"code","source":"# 定义BiGRU模型\nclass BiGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(BiGRU, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n        out, _ = self.gru(x, h0)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out\n\n# 生成数据样例\n# 均值为1的正态分布随机数\ndata_0 = np.random.randn(50, 20, 1) + 1\n# 均值为-1的正态分布随机数\ndata_1 = np.random.randn(50, 20, 1) - 1","metadata":{"execution":{"iopub.status.busy":"2023-10-07T08:00:43.491461Z","iopub.execute_input":"2023-10-07T08:00:43.491784Z","iopub.status.idle":"2023-10-07T08:00:43.511252Z","shell.execute_reply.started":"2023-10-07T08:00:43.491758Z","shell.execute_reply":"2023-10-07T08:00:43.509996Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_0.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-07T08:00:43.512786Z","iopub.execute_input":"2023-10-07T08:00:43.513085Z","iopub.status.idle":"2023-10-07T08:00:43.526768Z","shell.execute_reply.started":"2023-10-07T08:00:43.513060Z","shell.execute_reply":"2023-10-07T08:00:43.525790Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(50, 20, 1)"},"metadata":{}}]},{"cell_type":"code","source":"# 合并为总数据集\ndata = np.concatenate([data_0, data_1], axis=0)\n# 将 labels 修改为对应大小的数组\nlabels = np.concatenate([np.zeros((50, 1)), np.ones((50, 1))], axis=0)\n\n# 划分训练集和验证集\nX_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)\nX_val = torch.tensor(X_val, dtype=torch.float32)\ny_val = torch.tensor(y_val, dtype=torch.long)\n\n# 定义训练和验证函数\ndef train(model, device, X_train, y_train, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()\n    output = model(X_train.to(device))\n    loss = criterion(output, y_train.squeeze().to(device))\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef validate(model, device, X_val, y_val, criterion):\n    model.eval()\n    with torch.no_grad():\n        output = model(X_val.to(device))\n        loss = criterion(output, y_val.squeeze().to(device))\n    return loss.item()\n\n# 训练模型\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_size = 1\nhidden_size = 32\nnum_layers = 1\nnum_classes = 2\nnum_epochs = 10\nlearning_rate = 0.01\n\nmodel = BiGRU(input_size, hidden_size, num_layers, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, device, X_train, y_train, optimizer, criterion)\n    val_loss = validate(model, device, X_val, y_val, criterion)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n\n# 测试模型\ndef test(model, device, X_test, y_test):\n    model.eval()\n    with torch.no_grad():\n        output = model(X_test.to(device))\n        _, predicted = torch.max(output.data, 1)\n        correct = (predicted == y_test.squeeze().to(device)).sum().item()\n        accuracy = correct / y_test.size(0)\n    return accuracy\n\ntest_accuracy = test(model, device, X_val, y_val)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-07T08:00:43.528253Z","iopub.execute_input":"2023-10-07T08:00:43.529040Z","iopub.status.idle":"2023-10-07T08:00:43.988288Z","shell.execute_reply.started":"2023-10-07T08:00:43.529012Z","shell.execute_reply":"2023-10-07T08:00:43.987128Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch [1/10], Train Loss: 0.7353, Validation Loss: 0.6560\nEpoch [2/10], Train Loss: 0.6422, Validation Loss: 0.5884\nEpoch [3/10], Train Loss: 0.5605, Validation Loss: 0.5186\nEpoch [4/10], Train Loss: 0.4841, Validation Loss: 0.4433\nEpoch [5/10], Train Loss: 0.4079, Validation Loss: 0.3616\nEpoch [6/10], Train Loss: 0.3292, Validation Loss: 0.2744\nEpoch [7/10], Train Loss: 0.2481, Validation Loss: 0.1852\nEpoch [8/10], Train Loss: 0.1680, Validation Loss: 0.1057\nEpoch [9/10], Train Loss: 0.0990, Validation Loss: 0.0519\nEpoch [10/10], Train Loss: 0.0521, Validation Loss: 0.0248\nTest Accuracy: 100.00%\n","output_type":"stream"}]}]}