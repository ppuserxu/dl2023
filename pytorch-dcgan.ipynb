{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dset\nfrom torch.autograd import Variable","metadata":{"execution":{"iopub.status.busy":"2023-10-19T00:59:35.477449Z","iopub.execute_input":"2023-10-19T00:59:35.477848Z","iopub.status.idle":"2023-10-19T00:59:39.790728Z","shell.execute_reply.started":"2023-10-19T00:59:35.477814Z","shell.execute_reply":"2023-10-19T00:59:39.789789Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# 接下来，我们定义生成器和判别器的网络结构：\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # 输入是一个100维的向量\n            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            # 输出为(512, 4, 4)\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            # 输出为(256, 8, 8)\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            # 输出为(128, 16, 16)\n            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # 输出为(3, 32, 32)\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            # 输入为(3, 32, 32)\n            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 输出为(128, 16, 16)\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 输出为(256, 8, 8)\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 输出为(512, 4, 4)\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T00:59:39.792870Z","iopub.execute_input":"2023-10-19T00:59:39.793961Z","iopub.status.idle":"2023-10-19T00:59:39.810363Z","shell.execute_reply.started":"2023-10-19T00:59:39.793926Z","shell.execute_reply":"2023-10-19T00:59:39.809064Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 4. 数据样例\n\n# 我们将使用CIFAR-10数据集进行训练。首先，我们需要对数据进行预处理：\ntransform = transforms.Compose([\n    transforms.Resize(32),\n    transforms.CenterCrop(32),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrainset = dset.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n\n\n# 5. 训练模型\n\n# 接下来，我们将训练DCGAN模型：\n\n# 初始化生成器和判别器\nnetG = Generator()\nnetD = Discriminator()\n\n# 设置损失函数和优化器\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))","metadata":{"execution":{"iopub.status.busy":"2023-10-19T00:59:39.811904Z","iopub.execute_input":"2023-10-19T00:59:39.813160Z","iopub.status.idle":"2023-10-19T00:59:54.918973Z","shell.execute_reply.started":"2023-10-19T00:59:39.813119Z","shell.execute_reply":"2023-10-19T00:59:54.917888Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:10<00:00, 15738317.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\n","output_type":"stream"}]},{"cell_type":"code","source":"# 训练模型\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    for i, data in enumerate(trainloader, 0):\n        # 更新判别器\n        netD.zero_grad()\n        real, _ = data\n        batch_size = real.size(0)\n        label = torch.full((batch_size,), 1)\n        output = netD(real)\n        errD_real = criterion(output, label)\n        errD_real.backward()\n        noise = torch.randn(batch_size, 100, 1, 1)\n        fake = netG(noise)\n        label.fill_(0)\n        output = netD(fake.detach())\n        errD_fake = criterion(output, label)\n        errD_fake.backward()\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        # 更新生成器\n        netG.zero_grad()\n        label.fill_(1)\n        output = netD(fake)\n        errG = criterion(output, label)\n        errG.backward()\n        optimizerG.step()\n\n        if i%5==0:\n           # 打印损失值\n           print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, num_epochs, i, len(trainloader), errD.item(), errG.item()))\n\n    \n# 6. 测试模型\n\n# 训练完成后，我们可以使用生成器生成一些图像进行测试：\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\nnoise = torch.randn(64, 100, 1, 1)\nfake = netG(noise)\nimshow(torchvision.utils.make_grid(fake.detach()))\n\n   \n# 7. 总结\n\n# 本文详细介绍了DCGAN模型的原理，并使用PyTorch搭建了一个简单的DCGAN模型。我们提供了模型代码，并使用CIFAR-10数据集进行训练和测试。最后，我们展示了训练过程中的损失值和生成的图像","metadata":{"execution":{"iopub.status.busy":"2023-10-19T00:59:54.921448Z","iopub.execute_input":"2023-10-19T00:59:54.921992Z","iopub.status.idle":"2023-10-19T00:59:56.317913Z","shell.execute_reply.started":"2023-10-19T00:59:54.921964Z","shell.execute_reply":"2023-10-19T00:59:56.316384Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m netD(real)\n\u001b[0;32m---> 12\u001b[0m errD_real \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m errD_real\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3098\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3096\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"],"ename":"RuntimeError","evalue":"Found dtype Long but expected Float","output_type":"error"}]}]}