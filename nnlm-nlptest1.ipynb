{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:44:23.485111Z","iopub.execute_input":"2023-09-19T14:44:23.485517Z","iopub.status.idle":"2023-09-19T14:44:23.491399Z","shell.execute_reply.started":"2023-09-19T14:44:23.485486Z","shell.execute_reply":"2023-09-19T14:44:23.490132Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dtype = torch.FloatTensor\n\nsentences = [ \"i like dog\", \"i love coffee\", \"i hate milk\"]\n\nword_list = \" \".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nnumber_dict = {i: w for i, w in enumerate(word_list)}\nn_class = len(word_dict)  # 字典长度，number of Vocabulary\n\n# NNLM Parameter\nn_step = 2  # 窗口大小，n-1 in paper\nn_hidden = 2  # h in paper\nm = 2  # 最终词向量的长度，m in paper","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:44:26.352047Z","iopub.execute_input":"2023-09-19T14:44:26.352465Z","iopub.status.idle":"2023-09-19T14:44:26.372494Z","shell.execute_reply.started":"2023-09-19T14:44:26.352434Z","shell.execute_reply":"2023-09-19T14:44:26.371087Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"word_dict","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:45:21.514406Z","iopub.execute_input":"2023-09-19T14:45:21.515317Z","iopub.status.idle":"2023-09-19T14:45:21.523697Z","shell.execute_reply.started":"2023-09-19T14:45:21.515263Z","shell.execute_reply":"2023-09-19T14:45:21.522393Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'like': 0, 'milk': 1, 'dog': 2, 'i': 3, 'hate': 4, 'coffee': 5, 'love': 6}"},"metadata":{}}]},{"cell_type":"code","source":"number_dict","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:45:24.710556Z","iopub.execute_input":"2023-09-19T14:45:24.710955Z","iopub.status.idle":"2023-09-19T14:45:24.718106Z","shell.execute_reply.started":"2023-09-19T14:45:24.710925Z","shell.execute_reply":"2023-09-19T14:45:24.716898Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{0: 'like', 1: 'milk', 2: 'dog', 3: 'i', 4: 'hate', 5: 'coffee', 6: 'love'}"},"metadata":{}}]},{"cell_type":"code","source":"def make_batch(sentences):\n    # 其实就是将句子对应成各自的向量，然后input_batch为前面的词，target_batch为要预测的词\n    input_batch = []\n    target_batch = []\n\n    for sen in sentences:\n        word = sen.split()\n        input = [word_dict[n] for n in word[:-1]]\n        target = word_dict[word[-1]]\n#         print(\"input, lbl：\",input,target)\n\n        input_batch.append(input)\n        target_batch.append(target)\n\n    return input_batch, target_batch\n\n# Model\nclass NNLM(nn.Module):\n    def __init__(self):\n        super(NNLM, self).__init__()\n        self.C = nn.Embedding(n_class, m) #投影矩阵\n        self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype))  # 隐藏层的weight\n        self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(dtype))  # softmax层作用于词嵌入的weight\n        self.d = nn.Parameter(torch.randn(n_hidden).type(dtype))  # 隐藏层的bias\n        self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype))  # softmax层作用于hidden层的weight\n        self.b = nn.Parameter(torch.randn(n_class).type(dtype))  # softmax层的bias\n\n    def forward(self, X):\n        X = self.C(X)  # 这里巧妙的采用embedding，取出第X行刚好就是该词我们想要的词嵌入\n        X = X.view(-1, n_step * m)  # [数据个数即要训练样本数量, 窗口大小*词嵌入维度=每个训练样本的总的维度]\n        tanh = torch.tanh(self.d + torch.mm(X, self.H))  # 全连接层 [batch_size, n_hidden]\n        output = self.b + torch.mm(X, self.W) + torch.mm(tanh, self.U)  # softmax层[batch_size, n_class]\n        return output\n\nmodel = NNLM()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ninput_batch, target_batch = make_batch(sentences)\nprint(input_batch)\nprint(target_batch)\ninput_batch = torch.LongTensor(input_batch)  # embedded只能接受LongTensor\ntarget_batch = torch.LongTensor(target_batch)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:44:26.352047Z","iopub.execute_input":"2023-09-19T14:44:26.352465Z","iopub.status.idle":"2023-09-19T14:44:26.372494Z","shell.execute_reply.started":"2023-09-19T14:44:26.352434Z","shell.execute_reply":"2023-09-19T14:44:26.371087Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"input, lbl： [3, 0] 2\ninput, lbl： [3, 6] 5\ninput, lbl： [3, 4] 1\n[[3, 0], [3, 6], [3, 4]]\n[2, 5, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training\nfor epoch in range(5000):\n\n    optimizer.zero_grad()\n    output = model(input_batch)\n\n    loss = criterion(output, target_batch)\n    if (epoch + 1)%1000 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:44:26.555828Z","iopub.execute_input":"2023-09-19T14:44:26.556296Z","iopub.status.idle":"2023-09-19T14:44:30.643732Z","shell.execute_reply.started":"2023-09-19T14:44:26.556261Z","shell.execute_reply":"2023-09-19T14:44:30.642427Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch: 1000 cost = 0.335694\nEpoch: 2000 cost = 0.073772\nEpoch: 3000 cost = 0.023977\nEpoch: 4000 cost = 0.009994\nEpoch: 5000 cost = 0.004726\n","output_type":"stream"}]},{"cell_type":"code","source":"# Predict\npredict = model(input_batch).data.max(1, keepdim=True)[1] # .data只取出tensor中的数据，对于其他信息都丢弃","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:44:30.646468Z","iopub.execute_input":"2023-09-19T14:44:30.646906Z","iopub.status.idle":"2023-09-19T14:44:30.652428Z","shell.execute_reply.started":"2023-09-19T14:44:30.646866Z","shell.execute_reply":"2023-09-19T14:44:30.651590Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Test\nprint([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:44:30.653538Z","iopub.execute_input":"2023-09-19T14:44:30.654390Z","iopub.status.idle":"2023-09-19T14:44:30.665982Z","shell.execute_reply.started":"2023-09-19T14:44:30.654359Z","shell.execute_reply":"2023-09-19T14:44:30.665047Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['dog', 'coffee', 'milk']\n","output_type":"stream"}]}]}