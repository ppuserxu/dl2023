{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 首先，我们需要对数据进行预处理，包括分词、构建词汇表和生成训练数据。\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nimport numpy as np\n\n# 分词\ndef tokenize(text):\n    return text.lower().split()\n\n# 构建词汇表\ndef build_vocab(tokenized_text):\n    word_counts = Counter(tokenized_text)\n    vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n    return vocab","metadata":{"execution":{"iopub.status.busy":"2023-10-15T18:06:30.667778Z","iopub.execute_input":"2023-10-15T18:06:30.668107Z","iopub.status.idle":"2023-10-15T18:06:33.661380Z","shell.execute_reply.started":"2023-10-15T18:06:30.668082Z","shell.execute_reply":"2023-10-15T18:06:33.660131Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# 生成训练数据\nclass TextDataset(Dataset):\n    def __init__(self, text, vocab):\n        self.text = text\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        return self.text[idx], self.vocab[self.text[idx]]\n\ntext = \"I have a cat. She likes to play with her toys. My cat is very cute.\"\ntokenized_text = tokenize(text)\nvocab = build_vocab(tokenized_text)\ndataset = TextDataset(tokenized_text, vocab)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-15T18:06:33.663795Z","iopub.execute_input":"2023-10-15T18:06:33.664942Z","iopub.status.idle":"2023-10-15T18:06:35.296358Z","shell.execute_reply.started":"2023-10-15T18:06:33.664893Z","shell.execute_reply":"2023-10-15T18:06:35.295486Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"for batch in dataloader:\n    X, inputs = batch\n    print(X, inputs)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T18:14:02.152471Z","iopub.execute_input":"2023-10-15T18:14:02.152834Z","iopub.status.idle":"2023-10-15T18:14:02.160639Z","shell.execute_reply.started":"2023-10-15T18:14:02.152804Z","shell.execute_reply":"2023-10-15T18:14:02.159537Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"('play', 'very') tensor([ 7, 14])\n('cat', 'likes') tensor([12,  5])\n('cute.', 'a') tensor([15,  2])\n('cat.', 'her') tensor([3, 9])\n('i', 'have') tensor([0, 1])\n('is', 'my') tensor([13, 11])\n('toys.', 'to') tensor([10,  6])\n('she', 'with') tensor([4, 8])\n","output_type":"stream"}]},{"cell_type":"code","source":"# 3. 搭建ELMo模型\n\n# 接下来，我们将使用PyTorch搭建ELMo模型。模型包括一个词嵌入层、一个双向LSTM层和一个线性输出层。\n\nimport torch.nn as nn\n\nclass ELMo(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n        super(ELMo, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True)\n        self.linear = nn.Linear(hidden_dim * 2, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.lstm(x)\n        x = self.linear(x)\n        return x\n\nvocab_size = len(vocab)\nembedding_dim = 100\nhidden_dim = 128\nnum_layers = 2\nmodel = ELMo(vocab_size, embedding_dim, hidden_dim, num_layers)\n\n   \n\n# 4. 训练模型\n\n# 现在我们可以开始训练模型。我们将使用交叉熵损失函数和Adam优化器。\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        _, inputs = batch\n        inputs = torch.tensor(inputs).long()  # 将输入数据转换为张量\n        targets = torch.tensor(inputs).long()  # 将目标数据转换为张量\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n\n\n# 5. 预测\n\n# 训练完成后，我们可以使用模型进行预测。这里我们将预测一个简单的句子：“My cat likes to play.”\n\ndef predict(model, sentence, vocab):\n    tokenized_sentence = tokenize(sentence)\n    input_ids = [vocab[word] for word in tokenized_sentence]\n    inputs = torch.tensor(input_ids).unsqueeze(1)\n    outputs = model(inputs)\n    predictions = torch.argmax(outputs, dim=-1)\n    pred = [tokenized_text[x] for x in list(predictions.numpy().reshape(-1))]\n\n    return [word for word, _ in vocab.items() if word in pred ]\n\nsentence = \"My cat likes to play\"\npredictions = predict(model, sentence, vocab)\nprint(\"Predictions:\", predictions)\n\n\n# 6. 总结\n\n# 这篇文章主要介绍了如何使用PyTorch搭建ELMo模型，包括模型的原理、数据准备、模型搭建、训练和预测。我们提供了完整的代码实现，确保代码可运行且无错误。希望本文能帮助您理解ELMo模型并在自己的项目中应用，更多模型的运用技巧请持续关注。","metadata":{"execution":{"iopub.status.busy":"2023-10-15T18:06:33.663795Z","iopub.execute_input":"2023-10-15T18:06:33.664942Z","iopub.status.idle":"2023-10-15T18:06:35.296358Z","shell.execute_reply.started":"2023-10-15T18:06:33.664893Z","shell.execute_reply":"2023-10-15T18:06:35.295486Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/3278061493.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs = torch.tensor(inputs).long()  # 将输入数据转换为张量\n/tmp/ipykernel_32/3278061493.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  targets = torch.tensor(inputs).long()  # 将目标数据转换为张量\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 2.791206121444702\nEpoch 2/20, Loss: 2.774533748626709\nEpoch 3/20, Loss: 2.6169509887695312\nEpoch 4/20, Loss: 2.4948501586914062\nEpoch 5/20, Loss: 2.4127554893493652\nEpoch 6/20, Loss: 1.857100248336792\nEpoch 7/20, Loss: 1.6170330047607422\nEpoch 8/20, Loss: 1.3536661863327026\nEpoch 9/20, Loss: 0.8292950391769409\nEpoch 10/20, Loss: 0.2775568962097168\nEpoch 11/20, Loss: 0.14102372527122498\nEpoch 12/20, Loss: 0.08528187870979309\nEpoch 13/20, Loss: 0.09879791736602783\nEpoch 14/20, Loss: 0.021245460957288742\nEpoch 15/20, Loss: 0.036651767790317535\nEpoch 16/20, Loss: 0.02133701741695404\nEpoch 17/20, Loss: 0.04726295918226242\nEpoch 18/20, Loss: 0.021418007090687752\nEpoch 19/20, Loss: 0.01557416282594204\nEpoch 20/20, Loss: 0.02507207915186882\nPredictions: ['likes', 'to', 'play', 'my', 'cat']\n","output_type":"stream"}]}]}