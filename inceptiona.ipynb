{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.utils\nfrom torchvision import transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-08-24T15:59:12.591370Z","iopub.execute_input":"2023-08-24T15:59:12.591919Z","iopub.status.idle":"2023-08-24T15:59:12.599998Z","shell.execute_reply.started":"2023-08-24T15:59:12.591875Z","shell.execute_reply":"2023-08-24T15:59:12.598346Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 1.数据准备\nbatch_size = 64\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n# 做归一化和放长\n\n# 1.1: 加载数据\ntrain_dataset = datasets.MNIST(root='../dataset/mnist/',    # 获取数据\n                               train=True,                  # 表示获取数据集\n                               download=True,               # 若本地没有，则下载\n                               transform=transform)\n# 1.2: 按照batch_size划分成小样本\ntrain_loader = DataLoader(dataset=train_dataset,\n                          shuffle=True,\n                          batch_size=batch_size)\n\ntest_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)\ntest_loader = DataLoader(dataset=test_dataset, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T15:59:12.602999Z","iopub.execute_input":"2023-08-24T15:59:12.603590Z","iopub.status.idle":"2023-08-24T15:59:12.757599Z","shell.execute_reply.started":"2023-08-24T15:59:12.603550Z","shell.execute_reply":"2023-08-24T15:59:12.756061Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 2. 设计基础模型\nclass InceptionA(torch.nn.Module):\n    def __init__(self, inChannels):                                 # inChannels表输入通道数\n        super(InceptionA, self).__init__()\n        # 2.1 第一层池化 + 1*1卷积\n        self.branch1_1x1 = nn.Conv2d(in_channels=inChannels,  # 输入通道\n                                           out_channels=24,         # 输出通道\n                                           kernel_size=1)           # 卷积核大小1*1\n        # 2.2 第二层1*1卷积\n        self.branch2_1x1 = nn.Conv2d(inChannels, 16, kernel_size=1)\n\n        # 2.3 第三层\n        self.branch3_1_1x1 = nn.Conv2d(inChannels, 16, kernel_size=1)\n        self.branch3_2_5x5 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n        # padding=2,因为要保持输出的宽高保持一致\n\n        # 2.4 第四层\n        self.branch4_1_1x1 = nn.Conv2d(inChannels, 16, kernel_size=1)\n        self.branch4_2_3x3 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n        self.branch4_3_3x3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n\n    def forward(self, X_input):\n        # 第一层\n        branch1_pool = F.avg_pool2d(X_input,        # 输入\n                                    kernel_size=3,  # 池化层的核大小3*3\n                                    stride=1,       # 每次移动一步\n                                    padding=1)\n        branch1 = self.branch1_1x1(branch1_pool)\n        # 第二层\n        branch2 = self.branch2_1x1(X_input)\n        # 第三层\n        branch3_1= self.branch3_1_1x1(X_input)\n        branch3 = self.branch3_2_5x5(branch3_1)\n        # 第四层\n        branch4_1 = self.branch4_1_1x1(X_input)\n        branch4_2 = self.branch4_2_3x3(branch4_1)\n        branch4 = self.branch4_3_3x3(branch4_2)\n        # 输出\n        output = [branch2, branch3, branch4, branch1]\n        # (batch_size, channel, w, h)   dim=1: 即安装通道进行拼接。\n        # eg: (1, 2, 3, 4) 和 （1, 4, 3, 4）按照dim=1拼接，则拼接后的shape为（1, 2+4, 3,  4）\n        return torch.cat(output, dim=1)\n\n# 3. 整合模型\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n        self.conv_2 = nn.Conv2d(in_channels=88, out_channels=20,\n                                kernel_size=5)\n        self.inceptionA_1 = InceptionA(inChannels=10)\n        self.inceptionA_2 = InceptionA(inChannels=20)\n\n        self.maxPool = nn.MaxPool2d(kernel_size=2)\n        self.fullConnect = nn.Linear(in_features=1408,\n                                     out_features=10)\n\n    def forward(self, X_input):\n        batchSize = X_input.size(0)\n        # 第一层： 卷积\n        x = self.conv_1(X_input)    # 卷积\n        x = self.maxPool(x)         # 池化\n        x = F.relu(x)               # 激活\n        # 第二层： InceptionA\n        x = self.inceptionA_1(x)\n        # 第三层: 再卷积\n        x = self.conv_2(x)\n        x = self.maxPool(x)\n        x = F.relu(x)\n        # 第四层： 再InceptionA\n        x = self.inceptionA_2(x)\n        # 第五层，全连接层\n        x = x.view(batchSize, -1)\n        # 表示将（batch_size, channels, w, h）按照batch_size进行拉伸成shape=(batchSize, chanenls*w*h)\n        # eg: 原x.shape=(64, 2, 3, 4),调用 y =x.view(x.size(0), -1)后，y.shape = (64, 2*3*4)=(64, 24)\n        y_pred = self.fullConnect(x)\n\n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2023-08-24T15:59:12.759710Z","iopub.execute_input":"2023-08-24T15:59:12.760237Z","iopub.status.idle":"2023-08-24T15:59:12.809758Z","shell.execute_reply.started":"2023-08-24T15:59:12.760201Z","shell.execute_reply":"2023-08-24T15:59:12.808165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 4. 创建损失函数和优化器\nmodel = Net()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device=device)\nLoss = nn.CrossEntropyLoss()\n# 交叉熵损失，计算，# 如：y=[0, 1, 0],\n# y_pred=[0.1, 0.6, 0.3] -> 交叉熵损失= -sum{ yln[y_pred]} = 0 + (-ln(0.6)) + 0\noptimizer = optim.SGD(params=model.parameters(),    # 模型中需要被更新的可学习参数\n                      lr=0.01,                      # 学习率\n                      momentum=0.9)                 # 动量值，引入之后的梯度下降由 w_t = w_t_1  - lr*dw\n                                                    # 变为（1） v_t = momentum*v_t_1 + dw, (2) w_t = w_t_1 - lr*v_t","metadata":{"execution":{"iopub.status.busy":"2023-08-24T15:59:12.759710Z","iopub.execute_input":"2023-08-24T15:59:12.760237Z","iopub.status.idle":"2023-08-24T15:59:12.809758Z","shell.execute_reply.started":"2023-08-24T15:59:12.760201Z","shell.execute_reply":"2023-08-24T15:59:12.808165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-08-24T16:09:14.647859Z","iopub.execute_input":"2023-08-24T16:09:14.648349Z","iopub.status.idle":"2023-08-24T16:09:14.657229Z","shell.execute_reply.started":"2023-08-24T16:09:14.648315Z","shell.execute_reply":"2023-08-24T16:09:14.655910Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"# 5. 训练\ndef train(epoch):\n    running_loss = 0.0\n    for batch_index, data in enumerate(train_loader, 0):\n        X_input, Y_label = data\n        X_input, Y_label = X_input.to(device), Y_label.to(device)\n\n        optimizer.zero_grad()\n\n        y_pred = model.forward(X_input)\n        loss = Loss(y_pred, Y_label)    # 这里得出的loss,是batch_size=64的64个样本的平均值\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if batch_index % 300 == 299:\n            # 打印图片\n            # plt.imshow(X_input[0].resize(X_input.shape[2], X_input.shape[3]), cmap=\"Greys\")\n            # plt.title(\"batch_index={}, y={}, y_pred={} \".format(batch_index, Y_label[0], y_pred[0]))\n            # plt.show()\n\n            print('[%d, %5d] loss: %.3f' % (epoch, batch_index, running_loss / 300))\n            running_loss = 0.0","metadata":{"execution":{"iopub.status.busy":"2023-08-24T15:59:12.759710Z","iopub.execute_input":"2023-08-24T15:59:12.760237Z","iopub.status.idle":"2023-08-24T15:59:12.809758Z","shell.execute_reply.started":"2023-08-24T15:59:12.760201Z","shell.execute_reply":"2023-08-24T15:59:12.808165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 6. 训练\ndef test():\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in test_loader:\n            X_test_input, Y_test_label = data\n            X_test_input, Y_test_label = X_test_input.to(device), Y_test_label.to(device)\n            y_test_pred = model.forward(X_test_input)\n\n            _, predicted = torch.max(y_test_pred.data, dim=1)   # dim=1, 表示求出列的最大值； 返回两个数，第一个为该列最大值，第二个为最大值的行索引\n            total += Y_test_label.size(0)\n            correct += (predicted == Y_test_label).sum().item()\n    print('测试集正确率: %d %% ' % (100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2023-08-24T15:59:12.759710Z","iopub.execute_input":"2023-08-24T15:59:12.760237Z","iopub.status.idle":"2023-08-24T15:59:12.809758Z","shell.execute_reply.started":"2023-08-24T15:59:12.760201Z","shell.execute_reply":"2023-08-24T15:59:12.808165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"startTime = time.time()\nfor epoch in range(1):\n    train(epoch)\n    test()\nendTime = time.time()\nprint(\"GPU耗时： \", endTime-startTime)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T15:59:12.811525Z","iopub.execute_input":"2023-08-24T15:59:12.812378Z","iopub.status.idle":"2023-08-24T16:00:27.797076Z","shell.execute_reply.started":"2023-08-24T15:59:12.812340Z","shell.execute_reply":"2023-08-24T16:00:27.795571Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[0,   299] loss: 0.437\n[0,   599] loss: 0.099\n[0,   899] loss: 0.070\n测试集正确率: 97 % \nGPU耗时：  74.97761821746826\n","output_type":"stream"}]}]}