{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# coding: utf-8\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nimport torch.nn.functional as F\n\n\nclass lstm_model(nn.Module):\n    def __init__(self, vocab, hidden_size, num_layers, dropout=0.5):\n        super(lstm_model, self).__init__()\n        self.vocab = vocab  # 字符数据集\n        # 索引，字符\n        self.int_char = {i: char for i, char in enumerate(vocab)}\n        self.char_int = {char: i for i, char in self.int_char.items()}\n        # 对字符进行one-hot encoding\n        self.encoder = OneHotEncoder(sparse=True).fit(vocab.reshape(-1, 1))\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # lstm层\n        self.lstm = nn.LSTM(len(vocab), hidden_size, num_layers, batch_first=True, dropout=dropout)\n\n        # 全连接层\n        self.linear = nn.Linear(hidden_size, len(vocab))\n\n    def forward(self, sequence, hs=None):\n        out, hs = self.lstm(sequence, hs)  # lstm的输出格式（batch_size, sequence_length, hidden_size）\n        out = out.reshape(-1, self.hidden_size)  # 这里需要将out转换为linear的输入格式，即（batch_size * sequence_length, hidden_size）\n        output = self.linear(out)  # linear的输出格式，(batch_size * sequence_length, vocab_size)\n        return output, hs\n\n    def onehot_encode(self, data):\n        return self.encoder.transform(data)\n\n    def onehot_decode(self, data):\n        return self.encoder.inverse_transform(data)\n\n    def label_encode(self, data):\n        return np.array([self.char_int[ch] for ch in data])\n\n    def label_decode(self, data):\n        return np.array([self.int_char[ch] for ch in data])\n\n\ndef get_batches(data, batch_size, seq_len):\n    '''\n    :param data: 源数据，输入格式(num_samples, num_features)\n    :param batch_size: batch的大小\n    :param seq_len: 序列的长度（精度）\n    :return: （batch_size, seq_len, num_features）\n    '''\n    num_features = data.shape[1]\n    num_chars = batch_size * seq_len  # 一个batch_size的长度\n\n    num_batches = int(np.floor(data.shape[0] / num_chars))  # 计算出有多少个batches\n\n    need_chars = num_batches * num_chars  # 计算出需要的总字符量\n\n    targets = np.vstack((data[1:].A, data[0].A))  # 可能版本问题，取成numpy比较好reshape\n\n    inputs = data[:need_chars].A.astype(\"int\")  # 从原始数据data中截取所需的字符数量need_words\n    targets = targets[:need_chars]\n\n    targets = targets.reshape(batch_size, -1, num_features)\n    inputs = inputs.reshape(batch_size, -1, num_features)\n\n    for i in range(0, inputs.shape[1], seq_len):\n        x = inputs[:, i: i+seq_len]\n        y = targets[:, i: i+seq_len]\n        yield x, y  # 节省内存\n\n\ndef train(model, data, batch_size, seq_len, epochs, lr=0.01, valid=None):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    if valid is not None:\n        data = model.onehot_encode(data.reshape(-1, 1))\n        valid = model.onehot_encode(valid.reshape(-1, 1))\n    else:\n        data = model.onehot_encode(data.reshape(-1, 1))\n\n    train_loss = []\n    val_loss = []\n\n    for epoch in range(epochs):\n        model.train()\n        hs = None  # hs等于hidden_size隐藏层节点\n        train_ls = 0.0\n        val_ls = 0.0\n        ncount=0\n        for x, y in get_batches(data, batch_size, seq_len):\n            optimizer.zero_grad()\n            x = torch.tensor(x).float().to(device)\n            out, hs = model(x, hs)\n            hs = ([h.data for h in hs])\n            y = y.reshape(-1, len(model.vocab))\n            y = model.onehot_decode(y)\n            y = model.label_encode(y.squeeze())\n            y = torch.from_numpy(y).long().to(device)\n            loss = criterion(out, y.squeeze())\n            loss.backward()\n            optimizer.step()\n            train_ls += loss.item()\n            \n            ncount+=1\n            if ncount%100==0:\n                print(epoch,ncount,\"train...\")\n\n        if valid is not None:\n            model.eval()\n            hs = None\n            with torch.no_grad():\n                for x, y in get_batches(valid, batch_size, seq_len):\n                    x = torch.tensor(x).float().to(device)  # x为一组测试数据，包含batch_size * seq_len个字\n                    out, hs = model(x, hs)\n\n                    # out.shape输出为tensor[batch_size * seq_len, vocab_size]\n                    hs = ([h.data for h in hs])  # 更新参数\n\n                    y = y.reshape(-1, len(model.vocab))  # y.shape为(128,100,43)，因此需要转成两维，每行就代表一个字了，43为字典大小\n                    y = model.onehot_decode(y)  # y标签即为测试数据各个字的下一个字，进行one_hot解码，即变为字符\n                    # 但是此时y 是[[..],[..]]形式\n                    y = model.label_encode(y.squeeze())  # 因此需要去掉一维才能成功解码\n                    # 此时y为[12...]成为一维的数组，每个代表自己字典里对应字符的字典序\n                    y = torch.from_numpy(y).long().to(device)\n\n                    # 这里y和y.squeeze()出来的东西一样，可能这里没啥用，不太懂\n                    loss = criterion(out, y.squeeze())  # 计算损失值\n                    val_ls += loss.item()\n\n            val_loss.append(np.mean(val_ls))\n        train_loss.append(np.mean(train_ls))\n        print(\"train_loss:\", train_ls)\n\n    plt.plot(train_loss, label=\"train_loss\")\n    plt.plot(val_loss, label=\"val loss\")\n    plt.title(\"loop vs epoch\")\n    plt.legend()\n    plt.show()\n\n    model_name = \"lstm_model.net\"\n\n    with open(model_name, 'wb') as f:  # 训练完了保存模型\n        torch.save(model.state_dict(), f)\n\n\ndef predict(model, char, top_k=None, hidden_size=None):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    model.eval()  # 固定参数\n    with torch.no_grad():\n        char = np.array([char])  # 输入一个字符，预测下一个字是什么，先转成numpy\n        char = char.reshape(-1, 1)  # 变成二维才符合编码规范\n        char_encoding = model.onehot_encode(char).A  # 对char进行编码，取成numpy比较方便reshape\n        char_encoding = char_encoding.reshape(1, 1, -1)  # char_encoding.shape为(1, 1, 43)变成三维才符合模型输入格式\n        char_tensor = torch.tensor(char_encoding, dtype=torch.float32)  # 转成tensor\n        char_tensor = char_tensor.to(device)\n\n        out, hidden_size = model(char_tensor, hidden_size)  # 放入模型进行预测，out为结果\n\n        probs = F.softmax(out, dim=1).squeeze()  # 计算预测值,即所有字符的概率\n\n        if top_k is None:  # 选择概率最大的top_k个\n            indices = np.arange(vocab_size)\n        else:\n            probs, indices = probs.topk(top_k)\n            indices = indices.cpu().numpy()\n        probs = probs.cpu().numpy()\n\n        char_index = np.random.choice(indices, p=probs/probs.sum())  # 随机选择一个字符索引作为预测值\n        char = model.int_char[char_index]  # 通过索引找出预测字符\n\n    return char, hidden_size\n\n\ndef sample(model, length, top_k=None, sentence=\"c\"):\n    hidden_size = None\n    new_sentence = [char for char in sentence]\n    for i in range(length):\n        next_char, hidden_size = predict(model, new_sentence[-1], top_k=top_k, hidden_size=hidden_size)\n        new_sentence.append(next_char)\n    return \"\".join(new_sentence)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T13:39:36.837113Z","iopub.execute_input":"2023-10-15T13:39:36.837551Z","iopub.status.idle":"2023-10-15T13:39:40.963087Z","shell.execute_reply.started":"2023-10-15T13:39:36.837500Z","shell.execute_reply":"2023-10-15T13:39:40.961790Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"fname=\"/kaggle/input/noveleg2/noveleg.txt\"\nf=open(fname,\"r\").read()\nf[:100]","metadata":{"execution":{"iopub.status.busy":"2023-10-15T13:39:40.965106Z","iopub.execute_input":"2023-10-15T13:39:40.965761Z","iopub.status.idle":"2023-10-15T13:39:40.978270Z","shell.execute_reply.started":"2023-10-15T13:39:40.965720Z","shell.execute_reply":"2023-10-15T13:39:40.977313Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'嫌疑人X的献身\\n\\n东野圭吾\\n简介 \\n\\n    日本有史以来唯一一部囊括三大推理小说排行榜年度总冠军的顶尖杰作！\\n\\n    “这本小说了不起”第1名\\n\\n    “本格推理小说Top 10”第1名\\n\\n  '"},"metadata":{}}]},{"cell_type":"code","source":"# text = list(f)\n# text\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T13:39:40.979565Z","iopub.execute_input":"2023-10-15T13:39:40.980616Z","iopub.status.idle":"2023-10-15T13:39:40.989245Z","shell.execute_reply.started":"2023-10-15T13:39:40.980575Z","shell.execute_reply":"2023-10-15T13:39:40.987917Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def main():\n    hidden_size = 512\n    num_layers = 2\n    batch_size = 128\n    seq_len = 100\n    epochs = 2\n    lr = 0.01\n    \n#     f = pd.read_csv(\"../datasets/dev.tsv\", sep=\"\\t\", header=None)\n    fname=\"/kaggle/input/noveleg2/noveleg.txt\"\n    f=open(fname,\"r\").read()\n    \n#     f = f[0]\n    text = list(f)\n    text = \".\".join(text)\n    vocab = np.array(sorted(set(text)))  # 建立字典\n    vocab_size = len(vocab)\n\n    val_len = int(np.floor(0.2 * len(text)))  # 划分训练测试集\n    trainset = np.array(list(text[:-val_len]))\n    validset = np.array(list(text[-val_len:]))\n\n    model = lstm_model(vocab, hidden_size, num_layers)  # 模型实例化\n    train(model, trainset, batch_size, seq_len, epochs, lr=lr, valid=validset)  # 训练模型\n    model.load_state_dict(torch.load(\"lstm_model.net\"))  # 调用保存的模型\n    new_text = sample(model, 100, top_k=5)  # 预测模型，生成100个字符,预测时选择概率最大的前5个\n    print(new_text)  # 输出预测文本","metadata":{"execution":{"iopub.status.busy":"2023-10-15T13:39:40.992355Z","iopub.execute_input":"2023-10-15T13:39:40.992879Z","iopub.status.idle":"2023-10-15T13:39:41.003446Z","shell.execute_reply.started":"2023-10-15T13:39:40.992849Z","shell.execute_reply":"2023-10-15T13:39:41.002574Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2023-10-15T13:39:41.004947Z","iopub.execute_input":"2023-10-15T13:39:41.005558Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]}]}