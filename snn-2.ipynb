{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"使用PyTorch框架搭建一个简单的SNN模型。\n脉冲神经网络（SNN）是一种模拟生物神经元行为的神经网络模型，具有较高的计算效率和能量效率","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:22:47.919375Z","iopub.execute_input":"2023-08-20T10:22:47.920158Z","iopub.status.idle":"2023-08-20T10:22:52.361424Z","shell.execute_reply.started":"2023-08-20T10:22:47.920113Z","shell.execute_reply":"2023-08-20T10:22:52.359848Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"定义一个脉冲神经元（spiking neuron）类","metadata":{}},{"cell_type":"code","source":"class SpikingNeuron(nn.Module):\n    def __init__(self, threshold=1.0, decay=0.9):\n        super(SpikingNeuron, self).__init__()\n        self.threshold = threshold\n        self.decay = decay\n        self.membrane_potential = 0\n\n    def forward(self, x):\n        self.membrane_potential += x\n        spike = (self.membrane_potential >= self.threshold).float()\n        self.membrane_potential = self.membrane_potential * (1 - spike) * self.decay\n        return spike\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:22:52.363915Z","iopub.execute_input":"2023-08-20T10:22:52.364672Z","iopub.status.idle":"2023-08-20T10:22:52.372894Z","shell.execute_reply.started":"2023-08-20T10:22:52.364637Z","shell.execute_reply":"2023-08-20T10:22:52.371425Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 定义一个简单的SNN模型，包含一个输入层、一个隐藏层和一个输出层：\n\nclass SNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SNN, self).__init__()\n        self.input_layer = nn.Linear(input_size, hidden_size)\n        self.hidden_layer = SpikingNeuron()\n        self.output_layer = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.hidden_layer(x)\n        x = self.output_layer(x)\n        return x\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:22:52.374201Z","iopub.execute_input":"2023-08-20T10:22:52.374734Z","iopub.status.idle":"2023-08-20T10:22:52.398053Z","shell.execute_reply.started":"2023-08-20T10:22:52.374703Z","shell.execute_reply":"2023-08-20T10:22:52.396756Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 生成数据样例\nX = torch.randn(1000, 2)\ny = (X[:, 0] + X[:, 1] > 0).float()\n\n# 创建数据加载器\ndataset = data.TensorDataset(X, y)\ndata_loader = data.DataLoader(dataset, batch_size=10, shuffle=True)\n\n    \n\n\nmodel = SNN(input_size=2, hidden_size=10, output_size=1)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  ","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:22:52.401181Z","iopub.execute_input":"2023-08-20T10:22:52.401891Z","iopub.status.idle":"2023-08-20T10:22:52.546349Z","shell.execute_reply.started":"2023-08-20T10:22:52.401841Z","shell.execute_reply":"2023-08-20T10:22:52.545179Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"num_epochs = 200\n\nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    correct = 0\n    total = 0\n\n    for X_batch, y_batch in data_loader:\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs.view(-1), y_batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        correct += ((outputs.view(-1) > 0) == y_batch).sum().item()\n        total += y_batch.size(0)\n\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / total:.4f}, Accuracy: {correct / total:.4f}')\n\n    \n\n\n# 生成测试数据\nX_test = torch.randn(10, 2)\ny_test = (X_test[:, 0] + X_test[:, 1] > 0).float()\n\n# 测试模型\nwith torch.no_grad():\n    outputs = model(X_test)\n    test_loss = criterion(outputs.view(-1), y_test)\n    test_accuracy = ((outputs.view(-1) > 0) == y_test).sum().item() / y_test.size(0)\n\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:23:22.408083Z","iopub.execute_input":"2023-08-20T10:23:22.408521Z","iopub.status.idle":"2023-08-20T10:23:40.008290Z","shell.execute_reply.started":"2023-08-20T10:23:22.408486Z","shell.execute_reply":"2023-08-20T10:23:40.007077Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/200, Loss: 0.0639, Accuracy: 0.6280\nEpoch 2/200, Loss: 0.0566, Accuracy: 0.7680\nEpoch 3/200, Loss: 0.0540, Accuracy: 0.7610\nEpoch 4/200, Loss: 0.0519, Accuracy: 0.7590\nEpoch 5/200, Loss: 0.0514, Accuracy: 0.7740\nEpoch 6/200, Loss: 0.0505, Accuracy: 0.7720\nEpoch 7/200, Loss: 0.0499, Accuracy: 0.7780\nEpoch 8/200, Loss: 0.0514, Accuracy: 0.7620\nEpoch 9/200, Loss: 0.0509, Accuracy: 0.7740\nEpoch 10/200, Loss: 0.0495, Accuracy: 0.7720\nEpoch 11/200, Loss: 0.0505, Accuracy: 0.7680\nEpoch 12/200, Loss: 0.0523, Accuracy: 0.7520\nEpoch 13/200, Loss: 0.0503, Accuracy: 0.7700\nEpoch 14/200, Loss: 0.0472, Accuracy: 0.7990\nEpoch 15/200, Loss: 0.0488, Accuracy: 0.7780\nEpoch 16/200, Loss: 0.0486, Accuracy: 0.7830\nEpoch 17/200, Loss: 0.0486, Accuracy: 0.7870\nEpoch 18/200, Loss: 0.0508, Accuracy: 0.7650\nEpoch 19/200, Loss: 0.0514, Accuracy: 0.7610\nEpoch 20/200, Loss: 0.0475, Accuracy: 0.7940\nEpoch 21/200, Loss: 0.0495, Accuracy: 0.7680\nEpoch 22/200, Loss: 0.0498, Accuracy: 0.7770\nEpoch 23/200, Loss: 0.0492, Accuracy: 0.7850\nEpoch 24/200, Loss: 0.0528, Accuracy: 0.7610\nEpoch 25/200, Loss: 0.0490, Accuracy: 0.7820\nEpoch 26/200, Loss: 0.0491, Accuracy: 0.7790\nEpoch 27/200, Loss: 0.0502, Accuracy: 0.7730\nEpoch 28/200, Loss: 0.0490, Accuracy: 0.7790\nEpoch 29/200, Loss: 0.0507, Accuracy: 0.7660\nEpoch 30/200, Loss: 0.0510, Accuracy: 0.7680\nEpoch 31/200, Loss: 0.0478, Accuracy: 0.7890\nEpoch 32/200, Loss: 0.0502, Accuracy: 0.7670\nEpoch 33/200, Loss: 0.0530, Accuracy: 0.7640\nEpoch 34/200, Loss: 0.0494, Accuracy: 0.7770\nEpoch 35/200, Loss: 0.0511, Accuracy: 0.7560\nEpoch 36/200, Loss: 0.0502, Accuracy: 0.7630\nEpoch 37/200, Loss: 0.0511, Accuracy: 0.7640\nEpoch 38/200, Loss: 0.0515, Accuracy: 0.7640\nEpoch 39/200, Loss: 0.0497, Accuracy: 0.7730\nEpoch 40/200, Loss: 0.0504, Accuracy: 0.7630\nEpoch 41/200, Loss: 0.0506, Accuracy: 0.7680\nEpoch 42/200, Loss: 0.0492, Accuracy: 0.7780\nEpoch 43/200, Loss: 0.0491, Accuracy: 0.7900\nEpoch 44/200, Loss: 0.0504, Accuracy: 0.7690\nEpoch 45/200, Loss: 0.0518, Accuracy: 0.7640\nEpoch 46/200, Loss: 0.0510, Accuracy: 0.7650\nEpoch 47/200, Loss: 0.0497, Accuracy: 0.7800\nEpoch 48/200, Loss: 0.0493, Accuracy: 0.7760\nEpoch 49/200, Loss: 0.0506, Accuracy: 0.7720\nEpoch 50/200, Loss: 0.0471, Accuracy: 0.7900\nEpoch 51/200, Loss: 0.0523, Accuracy: 0.7540\nEpoch 52/200, Loss: 0.0493, Accuracy: 0.7770\nEpoch 53/200, Loss: 0.0497, Accuracy: 0.7760\nEpoch 54/200, Loss: 0.0511, Accuracy: 0.7620\nEpoch 55/200, Loss: 0.0485, Accuracy: 0.7800\nEpoch 56/200, Loss: 0.0506, Accuracy: 0.7680\nEpoch 57/200, Loss: 0.0501, Accuracy: 0.7710\nEpoch 58/200, Loss: 0.0472, Accuracy: 0.7960\nEpoch 59/200, Loss: 0.0482, Accuracy: 0.7840\nEpoch 60/200, Loss: 0.0505, Accuracy: 0.7580\nEpoch 61/200, Loss: 0.0503, Accuracy: 0.7770\nEpoch 62/200, Loss: 0.0482, Accuracy: 0.7870\nEpoch 63/200, Loss: 0.0474, Accuracy: 0.7910\nEpoch 64/200, Loss: 0.0500, Accuracy: 0.7700\nEpoch 65/200, Loss: 0.0496, Accuracy: 0.7730\nEpoch 66/200, Loss: 0.0481, Accuracy: 0.7830\nEpoch 67/200, Loss: 0.0493, Accuracy: 0.7860\nEpoch 68/200, Loss: 0.0502, Accuracy: 0.7750\nEpoch 69/200, Loss: 0.0510, Accuracy: 0.7640\nEpoch 70/200, Loss: 0.0499, Accuracy: 0.7680\nEpoch 71/200, Loss: 0.0482, Accuracy: 0.7820\nEpoch 72/200, Loss: 0.0480, Accuracy: 0.7890\nEpoch 73/200, Loss: 0.0507, Accuracy: 0.7700\nEpoch 74/200, Loss: 0.0491, Accuracy: 0.7700\nEpoch 75/200, Loss: 0.0507, Accuracy: 0.7650\nEpoch 76/200, Loss: 0.0501, Accuracy: 0.7750\nEpoch 77/200, Loss: 0.0489, Accuracy: 0.7770\nEpoch 78/200, Loss: 0.0479, Accuracy: 0.7920\nEpoch 79/200, Loss: 0.0493, Accuracy: 0.7750\nEpoch 80/200, Loss: 0.0508, Accuracy: 0.7660\nEpoch 81/200, Loss: 0.0504, Accuracy: 0.7660\nEpoch 82/200, Loss: 0.0502, Accuracy: 0.7590\nEpoch 83/200, Loss: 0.0494, Accuracy: 0.7760\nEpoch 84/200, Loss: 0.0490, Accuracy: 0.7830\nEpoch 85/200, Loss: 0.0483, Accuracy: 0.7940\nEpoch 86/200, Loss: 0.0510, Accuracy: 0.7680\nEpoch 87/200, Loss: 0.0496, Accuracy: 0.7830\nEpoch 88/200, Loss: 0.0512, Accuracy: 0.7650\nEpoch 89/200, Loss: 0.0494, Accuracy: 0.7750\nEpoch 90/200, Loss: 0.0515, Accuracy: 0.7570\nEpoch 91/200, Loss: 0.0514, Accuracy: 0.7630\nEpoch 92/200, Loss: 0.0506, Accuracy: 0.7760\nEpoch 93/200, Loss: 0.0495, Accuracy: 0.7750\nEpoch 94/200, Loss: 0.0496, Accuracy: 0.7830\nEpoch 95/200, Loss: 0.0479, Accuracy: 0.7920\nEpoch 96/200, Loss: 0.0467, Accuracy: 0.8070\nEpoch 97/200, Loss: 0.0484, Accuracy: 0.7800\nEpoch 98/200, Loss: 0.0482, Accuracy: 0.7830\nEpoch 99/200, Loss: 0.0516, Accuracy: 0.7620\nEpoch 100/200, Loss: 0.0500, Accuracy: 0.7700\nEpoch 101/200, Loss: 0.0493, Accuracy: 0.7810\nEpoch 102/200, Loss: 0.0477, Accuracy: 0.7950\nEpoch 103/200, Loss: 0.0503, Accuracy: 0.7640\nEpoch 104/200, Loss: 0.0496, Accuracy: 0.7780\nEpoch 105/200, Loss: 0.0510, Accuracy: 0.7650\nEpoch 106/200, Loss: 0.0496, Accuracy: 0.7780\nEpoch 107/200, Loss: 0.0500, Accuracy: 0.7710\nEpoch 108/200, Loss: 0.0498, Accuracy: 0.7790\nEpoch 109/200, Loss: 0.0489, Accuracy: 0.7800\nEpoch 110/200, Loss: 0.0487, Accuracy: 0.7920\nEpoch 111/200, Loss: 0.0507, Accuracy: 0.7670\nEpoch 112/200, Loss: 0.0504, Accuracy: 0.7790\nEpoch 113/200, Loss: 0.0504, Accuracy: 0.7740\nEpoch 114/200, Loss: 0.0461, Accuracy: 0.8000\nEpoch 115/200, Loss: 0.0501, Accuracy: 0.7690\nEpoch 116/200, Loss: 0.0471, Accuracy: 0.7930\nEpoch 117/200, Loss: 0.0515, Accuracy: 0.7650\nEpoch 118/200, Loss: 0.0510, Accuracy: 0.7710\nEpoch 119/200, Loss: 0.0482, Accuracy: 0.7870\nEpoch 120/200, Loss: 0.0498, Accuracy: 0.7770\nEpoch 121/200, Loss: 0.0486, Accuracy: 0.7890\nEpoch 122/200, Loss: 0.0499, Accuracy: 0.7770\nEpoch 123/200, Loss: 0.0508, Accuracy: 0.7590\nEpoch 124/200, Loss: 0.0504, Accuracy: 0.7690\nEpoch 125/200, Loss: 0.0509, Accuracy: 0.7660\nEpoch 126/200, Loss: 0.0534, Accuracy: 0.7480\nEpoch 127/200, Loss: 0.0513, Accuracy: 0.7600\nEpoch 128/200, Loss: 0.0487, Accuracy: 0.7860\nEpoch 129/200, Loss: 0.0496, Accuracy: 0.7760\nEpoch 130/200, Loss: 0.0490, Accuracy: 0.7740\nEpoch 131/200, Loss: 0.0489, Accuracy: 0.7800\nEpoch 132/200, Loss: 0.0489, Accuracy: 0.7810\nEpoch 133/200, Loss: 0.0498, Accuracy: 0.7740\nEpoch 134/200, Loss: 0.0491, Accuracy: 0.7860\nEpoch 135/200, Loss: 0.0517, Accuracy: 0.7690\nEpoch 136/200, Loss: 0.0508, Accuracy: 0.7650\nEpoch 137/200, Loss: 0.0498, Accuracy: 0.7750\nEpoch 138/200, Loss: 0.0508, Accuracy: 0.7690\nEpoch 139/200, Loss: 0.0474, Accuracy: 0.7960\nEpoch 140/200, Loss: 0.0493, Accuracy: 0.7860\nEpoch 141/200, Loss: 0.0512, Accuracy: 0.7620\nEpoch 142/200, Loss: 0.0500, Accuracy: 0.7760\nEpoch 143/200, Loss: 0.0494, Accuracy: 0.7920\nEpoch 144/200, Loss: 0.0509, Accuracy: 0.7650\nEpoch 145/200, Loss: 0.0480, Accuracy: 0.7880\nEpoch 146/200, Loss: 0.0514, Accuracy: 0.7660\nEpoch 147/200, Loss: 0.0492, Accuracy: 0.7850\nEpoch 148/200, Loss: 0.0511, Accuracy: 0.7630\nEpoch 149/200, Loss: 0.0498, Accuracy: 0.7760\nEpoch 150/200, Loss: 0.0487, Accuracy: 0.7770\nEpoch 151/200, Loss: 0.0479, Accuracy: 0.7820\nEpoch 152/200, Loss: 0.0511, Accuracy: 0.7630\nEpoch 153/200, Loss: 0.0503, Accuracy: 0.7720\nEpoch 154/200, Loss: 0.0496, Accuracy: 0.7820\nEpoch 155/200, Loss: 0.0502, Accuracy: 0.7590\nEpoch 156/200, Loss: 0.0494, Accuracy: 0.7700\nEpoch 157/200, Loss: 0.0523, Accuracy: 0.7550\nEpoch 158/200, Loss: 0.0514, Accuracy: 0.7640\nEpoch 159/200, Loss: 0.0513, Accuracy: 0.7640\nEpoch 160/200, Loss: 0.0493, Accuracy: 0.7750\nEpoch 161/200, Loss: 0.0488, Accuracy: 0.7900\nEpoch 162/200, Loss: 0.0498, Accuracy: 0.7630\nEpoch 163/200, Loss: 0.0506, Accuracy: 0.7700\nEpoch 164/200, Loss: 0.0512, Accuracy: 0.7610\nEpoch 165/200, Loss: 0.0502, Accuracy: 0.7620\nEpoch 166/200, Loss: 0.0503, Accuracy: 0.7690\nEpoch 167/200, Loss: 0.0495, Accuracy: 0.7760\nEpoch 168/200, Loss: 0.0470, Accuracy: 0.7910\nEpoch 169/200, Loss: 0.0500, Accuracy: 0.7750\nEpoch 170/200, Loss: 0.0519, Accuracy: 0.7620\nEpoch 171/200, Loss: 0.0490, Accuracy: 0.7730\nEpoch 172/200, Loss: 0.0519, Accuracy: 0.7540\nEpoch 173/200, Loss: 0.0511, Accuracy: 0.7580\nEpoch 174/200, Loss: 0.0482, Accuracy: 0.7910\nEpoch 175/200, Loss: 0.0504, Accuracy: 0.7730\nEpoch 176/200, Loss: 0.0495, Accuracy: 0.7790\nEpoch 177/200, Loss: 0.0502, Accuracy: 0.7680\nEpoch 178/200, Loss: 0.0503, Accuracy: 0.7670\nEpoch 179/200, Loss: 0.0492, Accuracy: 0.7860\nEpoch 180/200, Loss: 0.0500, Accuracy: 0.7660\nEpoch 181/200, Loss: 0.0489, Accuracy: 0.7820\nEpoch 182/200, Loss: 0.0518, Accuracy: 0.7610\nEpoch 183/200, Loss: 0.0512, Accuracy: 0.7640\nEpoch 184/200, Loss: 0.0492, Accuracy: 0.7800\nEpoch 185/200, Loss: 0.0478, Accuracy: 0.7870\nEpoch 186/200, Loss: 0.0500, Accuracy: 0.7720\nEpoch 187/200, Loss: 0.0492, Accuracy: 0.7750\nEpoch 188/200, Loss: 0.0485, Accuracy: 0.7760\nEpoch 189/200, Loss: 0.0493, Accuracy: 0.7730\nEpoch 190/200, Loss: 0.0471, Accuracy: 0.8020\nEpoch 191/200, Loss: 0.0498, Accuracy: 0.7740\nEpoch 192/200, Loss: 0.0505, Accuracy: 0.7740\nEpoch 193/200, Loss: 0.0487, Accuracy: 0.7870\nEpoch 194/200, Loss: 0.0480, Accuracy: 0.7860\nEpoch 195/200, Loss: 0.0514, Accuracy: 0.7680\nEpoch 196/200, Loss: 0.0503, Accuracy: 0.7740\nEpoch 197/200, Loss: 0.0509, Accuracy: 0.7680\nEpoch 198/200, Loss: 0.0484, Accuracy: 0.7870\nEpoch 199/200, Loss: 0.0511, Accuracy: 0.7630\nEpoch 200/200, Loss: 0.0486, Accuracy: 0.7780\nTest Loss: 0.5291, Test Accuracy: 0.8000\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}