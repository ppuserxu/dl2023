{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import namedtuple","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.692548Z","iopub.execute_input":"2023-11-11T12:26:13.693296Z","iopub.status.idle":"2023-11-11T12:26:13.698891Z","shell.execute_reply.started":"2023-11-11T12:26:13.693259Z","shell.execute_reply":"2023-11-11T12:26:13.697570Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"基于PyTorch实现Seq2Seq + Attention的英汉Neural Machine Translation","metadata":{}},{"cell_type":"markdown","source":"<!-- from collections import namedtuple\n# 命名元组对象student_info\nstudent_info = namedtuple('stud_info','name, id, gender, age, score')\n# 使用student_info对象对studinf进行赋值\nstudinf = student_info(name = 'xiaowang', id = '00001', gender = 'male', age = 22, score = 99)\nprint(\"name:{}, id:{}, gender:{}, age:{}, score:{}\".format(studinf[0],studinf[1],studinf[2],studinf[3],studinf[4])) -->","metadata":{}},{"cell_type":"markdown","source":"Pack_padded_sequence\n\n在编码句子向量的时候，通常会遇到 pack_padded_sequence 这样的函数，搞得一头雾水，重点理解下pack_padded_sequence 和 pad_packed_sequence 函数,他们是一对反函数\n\n实际含有函数pad_sequence + pack_padded_sequence = pack_sequence， 最后使用pad_packed_sequence 进行还原。\n\n为什么要填充：\n\n在进行mini-batch 喂入模型数据的时候要保证mini-batch中句子长度一致，但是对于较短的句子，就需要使用特定的字符进行填充到统一的句子长度。但是我们不希望其填充的pad数据（一般为0）进入GRU或是LSTM模块，一是浪费资源，二是可能造成句子表征不准确。所以pack_padded_sequence 类应运而生。主要是对填充过的数据进行压缩。","metadata":{}},{"cell_type":"code","source":"Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\nclass Encoder(nn.Module):\n    def __init__(self,vocab_size,embed_size,enc_hidden_size,dec_hidden_size,dropout=0.2):\n        super(Encoder,self).__init__()\n        self.embed = nn.Embedding(vocab_size,embed_size)\n\n        self.rnn = nn.GRU(embed_size,enc_hidden_size,batch_first=True,bidirectional=True)\n        self.dropout = nn.Dropout(dropout)\n        # 将encoder的输出转为decoder的输入，* 2 是使用了bidirectional\n        self.fc = nn.Linear(enc_hidden_size*2, dec_hidden_size)\n\n    def forward(self,x,lengths):\n        embedded = self.dropout(self.embed(x))\n\n        # 新版pytorch增加了batch里的排序功能，默认需要强制倒序\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,lengths,batch_first=True)\n        # hid 【2, batch, enc_hidden_size】\n        packed_out, hid = self.rnn(packed_embedded)\n        # 【batch, seq, 2 * enc_hidden_size】\n        out,_ = nn.utils.rnn.pad_packed_sequence(packed_out,batch_first=True,total_length=max(lengths))\n\n        # 将hid双向叠加 【batch, 2*enc_hidden_size】\n        hid = torch.cat([hid[-2],hid[-1]],dim=1)\n        # 转为decoder输入hidden state 【1,batch,dec_hidden_size】\n        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n\n        return out,hid\n\n\nclass Attention(nn.Module):\n    \"\"\"  \"\"\"\n    def __init__(self,enc_hidden_size,dec_hidden_size):\n        super(Attention,self).__init__()\n\n        self.enc_hidden_size = enc_hidden_size\n        self.dec_hidden_size = dec_hidden_size\n\n        self.liner_in = nn.Linear(2*enc_hidden_size,dec_hidden_size)\n        self.liner_out = nn.Linear(2*enc_hidden_size+dec_hidden_size,dec_hidden_size)\n\n    def forward(self,output,context,mask):\n        # context 上下文输出，即encoder的gru hidden state 【batch,enc_seq,enc_hidden*2】\n        # output  decoder的gru hidden state  【batch,dec_seq, dec_hidden】\n        # mask 【batch, dec_seq, enc_seq】mask在decoder中创建\n\n        batch_size = context.shape[0]\n        enc_seq = context.shape[1]\n        dec_seq = output.shape[1]\n\n        # score计算公式使用双线性模型 h*w*s\n        context_in = self.liner_in(context.reshape(batch_size*enc_seq,-1).contiguous())\n        context_in = context_in.view(batch_size,enc_seq,-1).contiguous()\n        atten = torch.bmm(output,context_in.transpose(1,2))\n        # 【batch,dec_seq,enc_seq】\n\n        atten.data.masked_fill(mask,-1e6)  # mask置零\n        atten = F.softmax(atten,dim=2)\n\n        # 将score和value加权求和，得到输出\n        # 【batch, dec_seq, 2*enc_hidden】\n        context = torch.bmm(atten,context)\n        # 将attention + output 堆叠获取融合信息\n        output = torch.cat((context,output),dim=2)\n\n        # 最终输出 batch,dec_seq,dec_hidden_size\n        output = torch.tanh(self.liner_out(output.view(batch_size*dec_seq,-1))).view(batch_size,dec_seq,-1)\n\n        return output,atten\n\n\nclass Decoder(nn.Module):\n    \"\"\"\"\"\"\n    def __init__(self,vocab_size,embedded_size,enc_hidden_size,dec_hidden_size,dropout=0.2):\n        super(Decoder,self).__init__()\n        self.embed = nn.Embedding(vocab_size,embedded_size)\n        self.atten = Attention(enc_hidden_size,dec_hidden_size)\n        # decoder不使用bidirectional\n        self.rnn = nn.GRU(embedded_size,dec_hidden_size,batch_first=True)\n        self.out = nn.Linear(dec_hidden_size,vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def create_mask(self,x_len,y_len):\n        # 最长句子的长度\n        max_x_len = x_len.max()\n        max_y_len = y_len.max()\n        # 句子batch\n        batch_size = len(x_len)\n\n        # 将超出自身序列长度的元素设为False\n        x_mask = (torch.arange(max_x_len.item())[None, :] < x_len[:, None]).float()  # [batch,max_x_len]\n        y_mask = (torch.arange(max_y_len.item())[None, :] < y_len[:, None]).float()  # [batch,max_y_len]\n\n        # y_mask[:, :, None] size: [batch,max_y_len,1]\n        # x_mask[:, None, :] size:  [batch,1,max_x_len]\n        # 需要mask的地方设置为true\n        mask = (1 - y_mask[:, :, None] * x_mask[:, None, :]) != 0\n\n        # [batch_size, max_y_len, max_x_len]\n        return mask\n\n    def forward(self,ctx,ctx_lengths,y,y_lengths,hid):\n        '''\n        :param ctx:encoder层的输出 ： 【batch, enc_seq, 2*enc_hidden】\n        :param ctx_lengths: encoder层输入句子的长度list\n        :param y: decoder层的输入 【batch, dec_seq, dec_hidden】\n        :param y_lengths: decoder输入的句子长度\n        :param hid: encoder层输出的最后一个hidden state 【1, batch, dec_hidden】\n        :return:\n        '''\n        y_embed = self.dropout(self.embed(y))\n        # 这里没法保证译文也是排倒序\n        y_packed = nn.utils.rnn.pack_padded_sequence(y_embed,y_lengths,batch_first=True,enforce_sorted=False)\n        # 将emcoder的hidden state作为decoder的第一个hidden state\n        pack_output, hid = self.rnn(y_packed,hid)\n        output_seq,_ = nn.utils.rnn.pad_packed_sequence(pack_output,batch_first=True,total_length=max(y_lengths))\n\n        # 做attention之前需要创建mask\n        mask = self.create_mask(ctx_lengths,y_lengths)\n        # annention处理\n        output,atten = self.atten(output_seq,ctx,mask)\n        # 将输出转为vocab_size的softmax概率分布并取对数\n        output = F.log_softmax(self.out(output),dim=-1)\n\n        return output,atten,hid","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.706716Z","iopub.execute_input":"2023-11-11T12:26:13.707111Z","iopub.status.idle":"2023-11-11T12:26:13.735542Z","shell.execute_reply.started":"2023-11-11T12:26:13.707081Z","shell.execute_reply":"2023-11-11T12:26:13.734437Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"class seq2seq(nn.Module):\n    '''\n        模型架构\n    '''\n    def __init__(self,encoder,decoder):\n        super(seq2seq,self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self,x,x_lengths,y,y_lengths):\n        context,hid = self.encoder(x,x_lengths)\n\n        output,atten,hid = self.decoder(\n            context,x_lengths,\n            y,y_lengths,\n            hid\n        )\n        # output: 【batch,output_len,vocab_size】\n        # atten   【batch,output_len,input_len】\n        return output,atten\n\n    def beam_search(self,x,x_lengths,y,EOS_id,topk=5,max_length=100):\n        encoder_out,hid = self.encoder(x,x_lengths)\n\n        # batch_size = x.shape[0]\n        # preds = []\n        # attens = []\n        # for i in range(max_length):\n        #\n        #     output,atten,hid = self.decoder(\n        #         encoder_out,x_lengths,\n        #         y,torch.ones(batch_size).long().to(y.device),\n        #         hid\n        #     )\n        #     # 取出预测概率最大index\n        #     y = output.argmax(2).view(batch_size,1)\n        #     preds.append(y)\n        #     attens.append(atten)\n\n        BOS_id = y[0][0].item()\n        hypotheses = [[BOS_id]]\n        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=y.device)\n        completed_hypotheses = []\n        t = 0\n        while len(completed_hypotheses) < topk and t < max_length:\n            t+=1\n            hyp_num = len(hypotheses)\n            # 扩展成batch\n            exp_src_encodings = encoder_out.expand(hyp_num,encoder_out.shape[1],encoder_out.shape[2])\n            exp_x_lengths = x_lengths.expand(hyp_num)\n            exp_hid = hid.expand(hid.shape[0],hyp_num,hid.shape[2])\n            output_t,atten_t,exp_hid = self.decoder(\n                exp_src_encodings,exp_x_lengths,\n                torch.tensor(hypotheses).long().to(y.device),torch.ones(hyp_num).long().to(y.device) * t,\n                exp_hid\n            )\n            live_hyp_num = topk - len(completed_hypotheses)\n\n            # 这里把num * vocab 展开来方便取topk\n            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand(hyp_num,output_t.shape[-1]) + output_t[:,-1,:].squeeze(1)).view(-1)\n            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores,k=live_hyp_num)\n\n            # 标记当前概率最大的k个，其是跟在哪个单词的后面\n            prev_hyp_ids = top_cand_hyp_pos // (output_t.shape[-1])\n            hyp_word_ids = top_cand_hyp_pos % (output_t.shape[-1])\n\n            new_hypotheses = []\n            live_hyp_ids = []\n            new_hyp_scores = []\n\n            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n                prev_hyp_id = prev_hyp_id.item()\n                hyp_word_id = hyp_word_id.item()\n                cand_new_hyp_score = cand_new_hyp_score.item()\n\n                # 将当前最大概率的k个，拼接在正确的prev单词后面\n                new_hyp_sent =  hypotheses[prev_hyp_id]  + [hyp_word_id]\n                if hyp_word_id == EOS_id:\n                    # 搜寻终止\n                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n                                                           score=cand_new_hyp_score))\n                else:\n                    new_hypotheses.append(new_hyp_sent)\n                    live_hyp_ids.append(prev_hyp_id)\n                    new_hyp_scores.append(cand_new_hyp_score)\n\n            if len(completed_hypotheses) == topk:\n                break\n\n            hypotheses = new_hypotheses\n            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=y.device)\n\n        # 若搜寻了max_len后还没有一个到达EOS则取第一个\n        if len(completed_hypotheses) == 0:\n            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n                                                   score=hyp_scores[0].item()))\n        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n\n        return completed_hypotheses\n\nclass LanguageModelCriterion(nn.Module):\n    def __init__(self):\n        '''损失函数'''\n        super(LanguageModelCriterion,self).__init__()\n\n    def forward(self,inuptY,target,mask):\n        # inputY batch,seq_len, vocab_size\n        # target/mask: batch, seq_len\n        inuptY = inuptY.contiguous().view(-1,inuptY.shape[2])\n        target = target.contiguous().view(-1,1)\n        mask = mask.contiguous().view(-1,1)\n        # 模型seq2seq的输出已经经过log-softmax了，只需将target对应index值收集后在mask\n        output = -inuptY.gather(1,target) * mask\n        return torch.sum(output) / torch.sum(mask)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.849851Z","iopub.execute_input":"2023-11-11T12:26:13.850310Z","iopub.status.idle":"2023-11-11T12:26:13.876247Z","shell.execute_reply.started":"2023-11-11T12:26:13.850276Z","shell.execute_reply":"2023-11-11T12:26:13.874861Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pkuseg\nimport nltk\nfrom nltk.translate.bleu_score import corpus_bleu\nimport argparse\nimport os\n\nfrom tqdm import trange,tqdm\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\nfrom transformers import AdamW,get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.878455Z","iopub.execute_input":"2023-11-11T12:26:13.878808Z","iopub.status.idle":"2023-11-11T12:26:13.890580Z","shell.execute_reply.started":"2023-11-11T12:26:13.878779Z","shell.execute_reply":"2023-11-11T12:26:13.889545Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# !pip install pkuseg","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.892301Z","iopub.execute_input":"2023-11-11T12:26:13.893257Z","iopub.status.idle":"2023-11-11T12:26:13.904869Z","shell.execute_reply.started":"2023-11-11T12:26:13.893223Z","shell.execute_reply":"2023-11-11T12:26:13.903668Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"<!-- ahshare, -->","metadata":{}},{"cell_type":"code","source":"# from model import Encoder,Attention,Decoder,seq2seq,LanguageModelCriterion\n\ndef setseed():\n    random.seed(2020)\n    np.random.seed(2020)\n    torch.manual_seed(2020)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(2020)\n\ndef load_file(path,tgt_add_bos=True):\n    en = []\n    cn = []\n    seg = pkuseg.pkuseg()\n    with open(path,'r') as f:\n        for line in f.readlines():\n            line = line.strip().split('\\t')\n            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n            # test时tgt不加开头结束，用于BLEU计算\n            if tgt_add_bos:\n                cn.append([\"BOS\"] + seg.cut(line[1]) + [\"EOS\"])\n            else:\n                cn.append(seg.cut(line[1]))\n\n    return en,cn\n\ndef build_tokenizer(sentences,args):\n    word_count = Counter()\n    for sen in sentences:\n        for word in sen:\n            word_count[word] += 1\n    ls = word_count.most_common(args.max_vocab_size)\n    word2idx = {word:idx+2 for idx,(word,_) in enumerate(ls)}\n    word2idx['UNK'] = args.UNK_IDX\n    word2idx['PAD'] = args.PAD_IDX\n\n    id2word = {v:k for k,v in word2idx.items()}\n    total_vocab = len(ls) + 2\n\n    return word2idx,id2word,total_vocab\n\ndef tokenize2num(en_sentences,cn_sentences,en_word2idx,cn_word2idx, sort_reverse = True):\n    length = len(en_sentences)\n\n    out_en_sents = [[en_word2idx.get(word,1) for word in sen] for sen in en_sentences]\n    out_cn_sents = [[cn_word2idx.get(word, 1) for word in sen] for sen in cn_sentences]\n\n    def sort_sents(sents):\n        return sorted(range(len(sents)),key = lambda x : len(sents[x]),reverse=True)\n    if sort_reverse:\n        sorted_index =  sort_sents(out_en_sents)\n        out_en_sents = [out_en_sents[idx] for idx in sorted_index]\n        out_cn_sents = [out_cn_sents[idx] for idx in sorted_index]\n\n    return out_en_sents,out_cn_sents\n\nclass Tokenizer(object):\n    def __init__(self,word2idx,id2word,vocab_size):\n        self.word2idx = word2idx\n        self.id2word = id2word\n        self.vocab_size = vocab_size","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.907424Z","iopub.execute_input":"2023-11-11T12:26:13.907970Z","iopub.status.idle":"2023-11-11T12:26:13.925813Z","shell.execute_reply.started":"2023-11-11T12:26:13.907938Z","shell.execute_reply":"2023-11-11T12:26:13.924635Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class DataProcessor(object):\n    def __init__(self,args):\n#         data_save\n        if not os.path.exists(args.data_save):os.makedirs(args.data_save)\n        \n        cached_en_tokenizer = os.path.join(args.data_save,\"cached_{}\".format(\"en_tokenizer\"))\n        cached_cn_tokenizer = os.path.join(args.data_save, \"cached_{}\".format(\"cn_tokenizer\"))\n\n        if not os.path.exists(cached_en_tokenizer) or not os.path.exists(cached_cn_tokenizer):\n            \n            en_sents, cn_sents = load_file(args.data_dir + \"train.txt\")\n            en_word2idx, en_id2word, en_vocab_size = build_tokenizer(en_sents,args)\n            cn_word2idx, cn_id2word, cn_vocab_size = build_tokenizer(cn_sents, args)\n\n            torch.save([en_word2idx, en_id2word, en_vocab_size],cached_en_tokenizer)\n            torch.save([cn_word2idx, cn_id2word, cn_vocab_size],cached_cn_tokenizer)\n        else:\n            en_word2idx, en_id2word, en_vocab_size = torch.load(cached_en_tokenizer)\n            cn_word2idx, cn_id2word, cn_vocab_size = torch.load(cached_cn_tokenizer)\n\n        self.en_tokenizer = Tokenizer(en_word2idx, en_id2word, en_vocab_size)\n        self.cn_tokenizer = Tokenizer(cn_word2idx, cn_id2word, cn_vocab_size)\n\n    def get_train_examples(self,args):\n        return self._create_examples(os.path.join(args.data_dir,\"train.txt\"),\"train\",args)\n\n\n    def get_dev_examples(self,args):\n        return self._create_examples(os.path.join(args.data_dir,\"dev.txt\"),\"dev\",args)\n\n    def _create_examples(self,path,set_type,args):\n        en_sents,cn_sents = load_file(path)\n        out_en_sents,out_cn_sents = tokenize2num(en_sents,cn_sents,\n                                                 self.en_tokenizer.word2idx,self.cn_tokenizer.word2idx)\n        minibatches = getminibatches(len(out_en_sents),args.batch_size)\n\n        all_examples = []\n        for minibatch in minibatches:\n            mb_en_sentences = [out_en_sents[i] for i in minibatch]\n            mb_cn_sentences = [out_cn_sents[i] for i in minibatch]\n\n            mb_x,mb_x_len = prepare_data(mb_en_sentences)\n            mb_y,mb_y_len = prepare_data(mb_cn_sentences)\n\n            all_examples.append((mb_x,mb_x_len,mb_y,mb_y_len))\n\n        return all_examples\n\ndef prepare_data(seqs):\n    # 处理每个batch句子（一个batch中句子长度可能不一致，需要pad）\n    batch_size = len(seqs)\n    lengthes = [len(seq) for seq in seqs]  # 每个句子的长度列表\n\n    max_length = max(lengthes)  # 句子最大长度\n    # 初始化句子矩阵都为0\n    x = np.zeros((batch_size, max_length)).astype(\"int32\")\n    for idx in range(batch_size):\n        # 按行将每行句子赋值进去\n        x[idx, :lengthes[idx]] = seqs[idx]\n\n    x_lengths = np.array(lengthes).astype(\"int32\")\n    return x, x_lengths\n\ndef getminibatches(n,batch_size,shuffle=True):\n    minibatches = np.arange(0,n,batch_size)\n    if shuffle:\n        np.random.shuffle(minibatches)\n\n    result = []\n    for idx in minibatches:\n        result.append(np.arange(idx,min(n,idx+batch_size)))\n    return result\n\n\ndef train(args,model, data,loss_fn,eval_data):\n    LOG_FILE = \"translation_model.log\"\n    tb_writer = SummaryWriter('./runs')\n\n    t_total = args.num_epoch * len(data)\n    optimizer = AdamW(model.parameters(), lr=args.learnning_rate, eps=1e-8)\n    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=args.warmup_steps,\n                                                num_training_steps=t_total)\n    global_step = 0\n    total_num_words = total_loss = 0.\n    logg_loss = 0.\n    logg_num_words = 0.\n    val_losses = []\n    train_iterator = trange(args.num_epoch,desc='epoch')\n    for epoch in train_iterator:\n        model.train()\n        epoch_iteration = tqdm(data, desc='iteration')\n        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(epoch_iteration):\n            # （英文batch，英文长度，中文batch，中文长度）\n            mb_x = torch.from_numpy(mb_x).to(args.device).long()\n            mb_x_len = torch.from_numpy(mb_x_len).to(args.device).long()\n            # 前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词\n            mb_input = torch.from_numpy(mb_y[:, :-1]).to(args.device).long()\n            mb_output = torch.from_numpy(mb_y[:, 1:]).to(args.device).long()\n            mb_y_len = torch.from_numpy(mb_y_len - 1).to(args.device).long()\n            # 输入输出的长度都减一。\n            mb_y_len[mb_y_len <= 0] = 1#?\n\n            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n            mb_out_mask = torch.arange(mb_y_len.max().item(), device=args.device)[None, :] < mb_y_len[:, None]\n            # batch,seq_len . 其中每行长度超过自身句子长度的为false\n            mb_out_mask = mb_out_mask.float()\n\n            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n            # 损失函数\n\n            # 更新模型\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.GRAD_CLIP)\n            # 为了防止梯度过大，设置梯度的阈值\n            optimizer.step()\n            scheduler.step()\n\n            global_step += 1\n            num_words = torch.sum(mb_y_len).item()\n            # 一个batch里多少个单词\n            total_loss += loss.item() * num_words\n            # 总损失，loss计算的是均值损失，每个单词都是都有损失，所以乘以单词数\n            total_num_words += num_words\n            # 总单词数\n\n            if (it+1) % 100 == 0:\n                loss_scalar = (total_loss - logg_loss) / (total_num_words-logg_num_words)\n                logg_num_words = total_num_words\n                logg_loss = total_loss\n\n                with open(LOG_FILE, \"a\") as fout:\n                    fout.write(\"epoch: {}, iter: {}, loss: {},learn_rate: {}\\n\".format(epoch, it, loss_scalar,\n                                                                                       scheduler.get_lr()[0]))\n                print(\"epoch: {}, iter: {}, loss: {}, learning_rate: {}\".format(epoch, it, loss_scalar,\n                                                                                scheduler.get_lr()[0]))\n                tb_writer.add_scalar(\"learning_rate\", scheduler.get_lr()[0], global_step)\n                tb_writer.add_scalar(\"loss\", loss_scalar, global_step)\n\n        print(\"Epoch\", epoch, \"Training loss\", total_loss / total_num_words)\n        eval_loss = evaluate(args, model, eval_data, loss_fn)  # 评估模型\n        with open(LOG_FILE, \"a\") as fout:\n            fout.write(\"===========\" * 20)\n            fout.write(\"EVALUATE: epoch: {}, loss: {}\\n\".format(epoch, eval_loss))\n        if len(val_losses) == 0 or eval_loss < min(val_losses):\n            # 如果比之前的loss要小，就保存模型\n            print(\"best model, val loss: \", eval_loss)\n            torch.save(model.state_dict(), \"translate-best.th\")\n        val_losses.append(eval_loss)\n\n\ndef evaluate(args,model, data,loss_fn):\n    model.eval()\n    total_num_words = total_loss = 0.\n    eval_iteration = tqdm(data, desc='eval iteration')\n    with torch.no_grad():#不需要更新模型，不需要梯度\n        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(eval_iteration):\n            mb_x = torch.from_numpy(mb_x).to(args.device).long()\n            mb_x_len = torch.from_numpy(mb_x_len).to(args.device).long()\n            mb_input = torch.from_numpy(mb_y[:, :-1]).to(args.device).long()\n            mb_output = torch.from_numpy(mb_y[:, 1:]).to(args.device).long()\n            mb_y_len = torch.from_numpy(mb_y_len-1).to(args.device).long()\n            mb_y_len[mb_y_len<=0] = 1\n\n            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n\n            mb_out_mask = torch.arange(mb_y_len.max().item(), device=args.device)[None, :] < mb_y_len[:, None]\n            mb_out_mask = mb_out_mask.float()\n\n            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n\n            num_words = torch.sum(mb_y_len).item()\n            total_loss += loss.item() * num_words\n            total_num_words += num_words\n    print(\"Evaluation loss\", total_loss/total_num_words)\n    return total_loss/total_num_words\n\ndef test(args,model,processor):\n    model.eval()\n    en_sents, cn_sents = load_file(args.data_dir+'test.txt',tgt_add_bos=False)\n    en_sents, _ = tokenize2num(en_sents, cn_sents,\n                               processor.en_tokenizer.word2idx,\n                               processor.cn_tokenizer.word2idx,\n                               sort_reverse=False)\n\n    top_hypotheses = []\n    test_iteration = tqdm(en_sents, desc='test bleu')\n    with torch.no_grad():\n        for idx, en_sent in enumerate(test_iteration):\n            mb_x = torch.from_numpy(np.array(en_sent).reshape(1, -1)).long().to(args.device)\n            mb_x_len = torch.from_numpy(np.array([len(en_sent)])).long().to(args.device)\n            bos = torch.Tensor([[processor.cn_tokenizer.word2idx['BOS']]]).long().to(args.device)\n            completed_hypotheses = model.beam_search(mb_x, mb_x_len,\n                                                     bos, processor.cn_tokenizer.word2idx['EOS'],\n                                                     topk=args.beam_size,\n                                                     max_length=args.max_beam_search_length)\n            top_hypotheses.append([processor.cn_tokenizer.id2word[id] for id in completed_hypotheses[0].value])\n\n    bleu_score = corpus_bleu([[ref] for ref in cn_sents],\n                             top_hypotheses)\n\n    print('Corpus BLEU: {}'.format(bleu_score * 100))\n\n    return bleu_score","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.927959Z","iopub.execute_input":"2023-11-11T12:26:13.928785Z","iopub.status.idle":"2023-11-11T12:26:13.985185Z","shell.execute_reply.started":"2023-11-11T12:26:13.928734Z","shell.execute_reply":"2023-11-11T12:26:13.983944Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"parse = argparse.ArgumentParser()\n#     default='./nmt/en-cn/'\nparse.add_argument(\"--data_dir\",default='/kaggle/input/encndata/data/',type=str,required=False,\n    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",)\nparse.add_argument(\"--batch_size\", default=16, type=int)\nparse.add_argument(\"--do_train\",default=True, action=\"store_true\", help=\"Whether to run training.\")\nparse.add_argument(\"--do_test\",default=True, action=\"store_true\", help=\"Whether to run test.\")\nparse.add_argument(\"--do_translate\",default=True, action=\"store_true\", help=\"Whether to run training.\")\nparse.add_argument(\"--learnning_rate\", default=5e-4, type=float)\nparse.add_argument(\"--dropout\", default=0.2, type=float)\nparse.add_argument(\"--num_epoch\", default=10, type=int)\nparse.add_argument(\"--max_vocab_size\",default=50000,type=int)\nparse.add_argument(\"--embed_size\",default=300,type=int)\nparse.add_argument(\"--enc_hidden_size\", default=512, type=int)\nparse.add_argument(\"--dec_hidden_size\", default=512, type=int)\nparse.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\nparse.add_argument(\"--GRAD_CLIP\", default=1, type=float)\nparse.add_argument(\"--UNK_IDX\",default=1,type=int)\nparse.add_argument(\"--PAD_IDX\", default=0, type=int)\nparse.add_argument(\"--beam_size\", default=5, type=int)\nparse.add_argument(\"--max_beam_search_length\", default=100, type=int)\n\nparse.add_argument(\"--data_save\",default='/kaggle/working/data/',type=str)\n\n#     args = parse.parse_args()\nargs = parse.parse_args(args=[]) \nprint(args)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:13.987741Z","iopub.execute_input":"2023-11-11T12:26:13.988186Z","iopub.status.idle":"2023-11-11T12:26:14.008139Z","shell.execute_reply.started":"2023-11-11T12:26:13.988070Z","shell.execute_reply":"2023-11-11T12:26:14.006607Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Namespace(data_dir='/kaggle/input/encndata/data/', batch_size=16, do_train=True, do_test=True, do_translate=True, learnning_rate=0.0005, dropout=0.2, num_epoch=10, max_vocab_size=50000, embed_size=300, enc_hidden_size=512, dec_hidden_size=512, warmup_steps=0, GRAD_CLIP=1, UNK_IDX=1, PAD_IDX=0, beam_size=5, max_beam_search_length=100, data_save='/kaggle/working/data/')\n","output_type":"stream"}]},{"cell_type":"code","source":"# def main():\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice=\"cpu\"\nargs.device = device\nprint(device)\n\nsetseed()\n\nprocessor = DataProcessor(args)\n\nencoder = Encoder(processor.en_tokenizer.vocab_size,args.embed_size,\n                  args.enc_hidden_size,args.dec_hidden_size,args.dropout)\ndecoder = Decoder(processor.cn_tokenizer.vocab_size,args.embed_size,\n                  args.enc_hidden_size,args.dec_hidden_size,args.dropout)\nmodel = seq2seq(encoder,decoder)\nif os.path.exists(\"translate-best.th\"):\n    model.load_state_dict(torch.load(\"translate-best.th\"))\nmodel.to(device)\nloss_fn = LanguageModelCriterion().to(device)\n\ntrain_data = processor.get_train_examples(args)\neval_data = processor.get_dev_examples(args)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:14.009206Z","iopub.execute_input":"2023-11-11T12:26:14.009560Z","iopub.status.idle":"2023-11-11T12:26:32.503320Z","shell.execute_reply.started":"2023-11-11T12:26:14.009530Z","shell.execute_reply":"2023-11-11T12:26:32.501783Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"#     if args.do_train:\n# train(args,model,train_data,loss_fn,eval_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:32.505060Z","iopub.execute_input":"2023-11-11T12:26:32.505502Z","iopub.status.idle":"2023-11-11T12:26:32.511081Z","shell.execute_reply.started":"2023-11-11T12:26:32.505465Z","shell.execute_reply":"2023-11-11T12:26:32.509716Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"#     if args.do_test:\ntest(args,model,processor)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:26:32.512762Z","iopub.execute_input":"2023-11-11T12:26:32.513148Z","iopub.status.idle":"2023-11-11T12:29:18.015828Z","shell.execute_reply.started":"2023-11-11T12:26:32.513115Z","shell.execute_reply":"2023-11-11T12:29:18.014836Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"test bleu: 100%|██████████| 1817/1817 [02:40<00:00, 11.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Corpus BLEU: 11.079267220946853\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"0.11079267220946853"},"metadata":{}}]},{"cell_type":"code","source":"#     if args.do_translate:\nmodel.load_state_dict(torch.load(\"translate-best.th\"))\nmodel.to(device)\nwhile True:\n    title = input(\"请输入要翻译的英文句子:\\n\")\n    if len(title.strip()) == 0:\n        continue\n    title = ['BOS'] + nltk.word_tokenize(title.lower()) + ['EOS']\n    title_num = [processor.en_tokenizer.word2idx.get(word,1) for word in title]\n    mb_x = torch.from_numpy(np.array(title_num).reshape(1,-1)).long().to(device)\n    mb_x_len = torch.from_numpy(np.array([len(title_num)])).long().to(device)\n\n    bos = torch.Tensor([[processor.cn_tokenizer.word2idx['BOS']]]).long().to(device)\n\n    completed_hypotheses = model.beam_search(mb_x, mb_x_len,\n                                             bos,processor.cn_tokenizer.word2idx['EOS'],\n                                             topk=args.beam_size,\n                                             max_length=args.max_beam_search_length)\n\n    for hypothes in completed_hypotheses:\n        result = \"\".join([processor.cn_tokenizer.id2word[id] for id in hypothes.value])\n        score = hypothes.score\n        print(\"翻译后的中文结果为:{},score:{}\".format(result,score))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T12:29:18.018619Z","iopub.execute_input":"2023-11-11T12:29:18.018972Z","iopub.status.idle":"2023-11-11T13:38:15.828845Z","shell.execute_reply.started":"2023-11-11T12:29:18.018928Z","shell.execute_reply":"2023-11-11T13:38:15.827196Z"},"trusted":true},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdin","text":"请输入要翻译的英文句子:\n i love you china\n"},{"name":"stdout","text":"翻译后的中文结果为:我愛你。,score:-2.6588497161865234\n翻译后的中文结果为:我愛你了。,score:-3.492776870727539\n翻译后的中文结果为:我想你愛。,score:-5.427981376647949\n翻译后的中文结果为:我对你愛。,score:-5.788558483123779\n翻译后的中文结果为:我对你愛了。,score:-6.098419189453125\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"请输入要翻译的英文句子:\n long long ago there lived in hangzhou a girl named aqiao\n"},{"name":"stdout","text":"翻译后的中文结果为:那本书在那裡有一個大超市。,score:-12.923996925354004\n翻译后的中文结果为:那本书在那裡有一個大問題。,score:-13.353044509887695\n翻译后的中文结果为:那本书在那裡有一個有趣的書。,score:-14.243057250976562\n翻译后的中文结果为:那本书在那裡有一本關於鳥類的書。,score:-16.189922332763672\n翻译后的中文结果为:那本书在那裡有一本關於鳥類的書在这里。,score:-18.45274543762207\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"请输入要翻译的英文句子:\n i love you more than i can say\n"},{"name":"stdout","text":"翻译后的中文结果为:我可以跟你说法语。,score:-6.410440921783447\n翻译后的中文结果为:我比你想说得好嗎？,score:-7.209282398223877\n翻译后的中文结果为:我可以跟你说得好嗎？,score:-7.415818691253662\n翻译后的中文结果为:我比你想说得好吗？,score:-7.553323745727539\n翻译后的中文结果为:我比你想知道我的好嗎？,score:-8.470877647399902\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"请输入要翻译的英文句子:\n \n请输入要翻译的英文句子:\n hello everyone\n"},{"name":"stdout","text":"翻译后的中文结果为:停火。,score:-3.715897560119629\n翻译后的中文结果为:鬼魂。,score:-4.276006698608398\n翻译后的中文结果为:停火是谁。,score:-5.814101696014404\n翻译后的中文结果为:停火是谁的。,score:-5.817572593688965\n翻译后的中文结果为:鬼魂是谁。,score:-5.86874532699585\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m请输入要翻译的英文句子:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(title\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}]},{"cell_type":"code","source":"# code ,github.","metadata":{},"execution_count":null,"outputs":[]}]}