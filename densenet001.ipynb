{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport torch.nn.functional as F\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:34.130494Z","iopub.execute_input":"2023-09-17T03:31:34.131046Z","iopub.status.idle":"2023-09-17T03:31:39.261351Z","shell.execute_reply.started":"2023-09-17T03:31:34.131008Z","shell.execute_reply":"2023-09-17T03:31:39.260251Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# transforms.RandomHorizontalFlip(p=0.5)---以0.5的概率对图片做水平横向翻转\ntransform_train = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n\n# transforms.ToTensor()---shape从(H,W,C)->(C,H,W), 每个像素点从(0-255)映射到(0-1):直接除以255\n# transforms.Normalize---先将输入归一化到(0,1),像素点通过\"(x-mean)/std\",将每个元素分布到(-1,1)\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize(std=(0.485, 0.456, 0.406), mean=(0.226, 0.224, 0.225))])\n\ntrain_dataset = datasets.CIFAR10(root=\"../DataSet/cifar10\", train=True, transform=transform_train,\n                                 download=True)\ntest_dataset = datasets.CIFAR10(root=\"../DataSet/cifar10\", train=False, transform=transform,\n                                download=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:39.263778Z","iopub.execute_input":"2023-09-17T03:31:39.264549Z","iopub.status.idle":"2023-09-17T03:31:50.288131Z","shell.execute_reply.started":"2023-09-17T03:31:39.264513Z","shell.execute_reply":"2023-09-17T03:31:50.287114Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../DataSet/cifar10/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:05<00:00, 28925124.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ../DataSet/cifar10/cifar-10-python.tar.gz to ../DataSet/cifar10\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    def __init__(self, input_channel, growth_rate):\n        super(Bottleneck, self).__init__()\n\n        self.bn1 = nn.BatchNorm2d(input_channel)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv1 = nn.Conv2d(input_channel, 4 * growth_rate, kernel_size=1)\n        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        out = self.conv1(self.relu1(self.bn1(x)))\n        out = self.conv2(self.relu2(self.bn2(out)))\n        out = torch.cat([out, x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, input_channels, out_channels):\n        super(Transition, self).__init__()\n\n        self.bn = nn.BatchNorm2d(input_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(input_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        out = self.conv(self.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, nblocks, growth_rate, reduction, num_classes):\n        super(DenseNet, self).__init__()\n\n        self.growth_rate = growth_rate\n\n        num_planes = 2 * growth_rate\n\n        self.basic_conv = nn.Sequential(\n\n            nn.Conv2d(3, 2 * growth_rate, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(2 * growth_rate),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        self.dense1 = self._make_dense_layers(num_planes, nblocks[0])\n        num_planes += nblocks[0] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(num_planes, nblocks[1])\n        num_planes += nblocks[1] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(num_planes, nblocks[2])\n        num_planes += nblocks[2] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(num_planes, nblocks[3])\n        num_planes += nblocks[3] * growth_rate\n\n        self.AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d(1)\n\n        # 全连接层\n        self.fc = nn.Sequential(\n\n            nn.Linear(num_planes, 256),\n            nn.ReLU(inplace=True),\n            # 使一半的神经元不起作用，防止参数量过大导致过拟合\n            nn.Dropout(0.5),\n\n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n\n            nn.Linear(128, 10)\n        )\n\n    def _make_dense_layers(self, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(Bottleneck(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.basic_conv(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = self.AdaptiveAvgPool2d(out)\n\n        out = out.view(out.size(0), -1)\n        \n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:50.289970Z","iopub.execute_input":"2023-09-17T03:31:50.290338Z","iopub.status.idle":"2023-09-17T03:31:50.313452Z","shell.execute_reply.started":"2023-09-17T03:31:50.290303Z","shell.execute_reply":"2023-09-17T03:31:50.312336Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def DenseNet121():\n    return DenseNet([6, 12, 24, 16], growth_rate=32, reduction=0.5, num_classes=10)\n\n\ndef DenseNet169():\n    return DenseNet([6, 12, 32, 32], growth_rate=32, reduction=0.5, num_classes=10)\n\n\ndef DenseNet201():\n    return DenseNet([6, 12, 48, 32], growth_rate=32, reduction=0.5, num_classes=10)\n\n\ndef DenseNet265():\n    return DenseNet([6, 12, 64, 48], growth_rate=32, reduction=0.5, num_classes=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:50.316559Z","iopub.execute_input":"2023-09-17T03:31:50.317028Z","iopub.status.idle":"2023-09-17T03:31:50.324612Z","shell.execute_reply.started":"2023-09-17T03:31:50.316969Z","shell.execute_reply":"2023-09-17T03:31:50.323575Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 初始化模型\nmodel = DenseNet121().to(device)\n\n# 构造损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\nopt = optim.SGD(model.parameters(), lr=0.01, momentum=0.8, weight_decay=0.001)\n\n# 动态更新学习率------每隔step_size : lr = lr * gamma\nschedule = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.6, last_epoch=-1)\n\nloss_list = []","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:50.326071Z","iopub.execute_input":"2023-09-17T03:31:50.327134Z","iopub.status.idle":"2023-09-17T03:31:53.343982Z","shell.execute_reply.started":"2023-09-17T03:31:50.327101Z","shell.execute_reply":"2023-09-17T03:31:53.342987Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Test\ndef verify():\n    model.eval()\n    correct = 0.0\n    total = 0\n    # 训练模式不需要反向传播更新梯度\n    with torch.no_grad():\n        print(\"===========================test===========================\")\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n\n            pred = outputs.argmax(dim=1)  # 返回每一行中最大值元素索引\n            total += inputs.size(0)\n            correct += torch.eq(pred, labels).sum().item()\n\n    print(\"Accuracy of the network on the 10000 test images:%.2f %%\" % (100 * correct / total))\n    print(\"==========================================================\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train\ndef train(epoch):\n    start = time.time()\n    for epoch in range(epoch):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(train_loader, 0):\n            # 重置梯度\n            opt.zero_grad()\n            \n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # 将数据送入模型训练\n            outputs = model(inputs)\n            # 计算损失\n            loss = criterion(outputs, labels).to(device)\n\n            \n            # 计算梯度，反向传播\n            loss.backward()\n            # 根据反向传播的梯度值优化更新参数\n            opt.step()\n\n            # 100个batch的 loss 之和\n            running_loss += loss.item()\n            loss_list.append(loss.item())\n\n            # 每100个 batch 查看一下 平均loss\n            if (i + 1) % 100 == 0:\n                print('epoch = %d , batch = %d , loss = %.6f' % (epoch + 1, i + 1, running_loss / 100))\n                running_loss = 0.0\n\n        # 每一轮结束输出一下当前的学习率 lr\n        lr_1 = opt.param_groups[0]['lr']\n        print(\"learn_rate:%.15f\" % lr_1)\n        schedule.step()\n        verify()\n\n    end = time.time()\n    # 计算并打印输出你的训练时间\n    print(\"time:{}\".format(end - start))\n\n    # 训练过程可视化\n    plt.plot(loss_list)\n    plt.ylabel('loss')\n    plt.xlabel('Epoch')\n    plt.savefig('./DenseNet_train_img.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:53.345288Z","iopub.execute_input":"2023-09-17T03:31:53.348150Z","iopub.status.idle":"2023-09-17T03:31:53.359755Z","shell.execute_reply.started":"2023-09-17T03:31:53.348113Z","shell.execute_reply":"2023-09-17T03:31:53.358832Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:53.361285Z","iopub.execute_input":"2023-09-17T03:31:53.361642Z","iopub.status.idle":"2023-09-17T03:31:53.794365Z","shell.execute_reply.started":"2023-09-17T03:31:53.361608Z","shell.execute_reply":"2023-09-17T03:31:53.793242Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train(100)\nverify()\n# DenseNet: 所有卷积层全部使用使用3*3的卷积核, 两个3*3=一个5*5 同时可以减少参数量, 加深神经网络的深度\n# 使用 DenseNet 神经网络训练 CIFAR10 数据集","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:31:53.796025Z","iopub.execute_input":"2023-09-17T03:31:53.796748Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"epoch = 1 , batch = 100 , loss = 2.285730\nepoch = 1 , batch = 200 , loss = 2.078401\nepoch = 1 , batch = 300 , loss = 1.860263\nepoch = 1 , batch = 400 , loss = 1.731221\nepoch = 1 , batch = 500 , loss = 1.674039\nepoch = 1 , batch = 600 , loss = 1.618481\nepoch = 1 , batch = 700 , loss = 1.578846\nlearn_rate:0.010000000000000\n===========================test===========================\nAccuracy of the network on the 10000 test images:21.15 %\n==========================================================\nepoch = 2 , batch = 100 , loss = 1.553077\n","output_type":"stream"}]}]}